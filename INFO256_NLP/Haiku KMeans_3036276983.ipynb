{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8780af9",
   "metadata": {},
   "source": [
    "In \"[Literary Pattern Recognition](https://www.journals.uchicago.edu/doi/full/10.1086/684353)\", Long and So train a classifier to differentiate haiku poems from non-haiku poems, and find that many features help do so.  In class, we've discussed the importance of representation--how you *describe* a text computationally influences the kinds of things you are able to do with it.  While Long and So explore description in the context of classification, in this homework, you'll see how well you can design features that can differentiate these two classes *without* any supervision. Are you able to featurize a collection of poems such that two clusters (haiku/non-haiku) emerge when using KMeans clustering, with the text representation as your only degree of freedom?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d30ae833",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv, os, re\n",
    "import nltk\n",
    "from scipy import sparse\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import metrics\n",
    "import math\n",
    "from collections import Counter\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "dced9002",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, operator, math, nltk\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f517024",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_texts(path, metadata, filepath_col):\n",
    "    data=[]\n",
    "    with open(metadata, encoding=\"utf-8\") as file:\n",
    "        csv_reader = csv.reader(file)\n",
    "        next(csv_reader)\n",
    "        for cols in csv_reader:\n",
    "            poem_path=os.path.join(path, cols[filepath_col])\n",
    "            if os.path.exists(poem_path):\n",
    "                with open(poem_path, encoding=\"utf-8\") as poem_file:\n",
    "                    poem=poem_file.read()\n",
    "                    data.append(poem)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d81026d2",
   "metadata": {},
   "source": [
    "Here we'll use data originally released on Github to support \"Literary Pattern Recognition\": [https://github.com/hoytlong/PatternRecognition](https://github.com/hoytlong/PatternRecognition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cfbe9556",
   "metadata": {},
   "outputs": [],
   "source": [
    "haiku=read_texts(\"../data/haiku/long_so_haiku\", \"../data/haiku/Haikus.csv\", 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "336544fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "others=read_texts(\"../data/haiku/long_so_others\", \"../data/haiku/OthersData.csv\", 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f2b4d33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# don't change anything within this code block\n",
    "\n",
    "def run_all(haiku, others, feature_function):\n",
    "    \n",
    "    #use thefunction to get the X(feature set position), y(output truth), (feature vocab)\n",
    "    X, Y, featurize_vocab=feature_function(haiku, others)\n",
    "    #use kmeans to cluster X\n",
    "    kmeans = KMeans(n_clusters=2, random_state=0).fit(X)\n",
    "    #calculate the distance between the labeled k means based on the feature function and output truth\n",
    "    nmi=metrics.normalized_mutual_info_score(Y, kmeans.labels_)\n",
    "    print(\"%.3f NMI\" % nmi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d2c16b1",
   "metadata": {},
   "source": [
    "As one example, let's take a simple featurization and represent each poem by a binary indicator of the dictionary word types it contains.  \"To be or not to be\", for example, would be represented as {\"to\": 1, \"be\": 1, \"or\": 1, \"not\": 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f433360a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function takes in a list of haiku poems and non-haiku poems, and returns:\n",
    "\n",
    "# X (sparse matrix, with poems as rows and features as columns)\n",
    "# Y (list of poem labels, with 1=haiku and 0=non-haiku)\n",
    "# feature_vocab (dict mapping feature name to feature ID)\n",
    "\n",
    "def unigram_featurize_all(haiku, others):\n",
    "\n",
    "    def unigram_featurize(poem, feature_vocab):\n",
    "        \n",
    "        # featurize text by just noting the binary presence of words within it\n",
    "        \n",
    "        feats={}\n",
    "\n",
    "        tokens=nltk.word_tokenize(poem.lower())\n",
    "        for token in tokens:\n",
    "            if token not in feature_vocab:\n",
    "                feature_vocab[token]=len(feature_vocab)\n",
    "            feats[feature_vocab[token]]=1\n",
    "        return feats\n",
    "\n",
    "    feature_vocab={}\n",
    "    data=[]\n",
    "    Y=[]\n",
    "\n",
    "    for poem in haiku:\n",
    "        feats=unigram_featurize(poem, feature_vocab)\n",
    "        data.append(feats)\n",
    "        Y.append(1)\n",
    "    for poem in others:\n",
    "        feats=unigram_featurize(poem, feature_vocab)\n",
    "        data.append(feats)\n",
    "        Y.append(0)\n",
    "    \n",
    "    # since the data above has all haiku ordered before non-haiku, let's shuffle them\n",
    "    temp = list(zip(data, Y))\n",
    "    random.shuffle(temp)\n",
    "    data, Y = zip(*temp)\n",
    "\n",
    "    # we'll use a sparse representation since our features are sparse\n",
    "    X=sparse.lil_matrix((len(data), len(feature_vocab)))\n",
    "\n",
    "    for idx,feats in enumerate(data):\n",
    "        for f in feats:\n",
    "            X[idx,f]=feats[f]\n",
    "    \n",
    "    return X, Y, feature_vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e695d6fb",
   "metadata": {},
   "source": [
    "This method yields an NMI of ~0.07 (with some variability due to the randomness of KMeans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a52279bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\cheny\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2b420ead",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.073 NMI\n"
     ]
    }
   ],
   "source": [
    "run_all(haiku, others, unigram_featurize_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e57de6",
   "metadata": {},
   "source": [
    "**Q1**: Copy the `unigram_featurize_all` code above and adapt it to create your own featurization method named `fancy_featurize_all`.  You may use whatever information you like to represent these poems for the purposes of clustering them into two categories, but you must use the KMeans clustering (with 2 clusters) as defined in `run_all`.  Use your own understanding of haiku, or read the Long and So article above for other ideas.  Are you able to improve over an NMI of 0.07?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "4b055e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fancy_featurize_log(haiku, others):\n",
    "    \n",
    "    # your code here\n",
    "    \n",
    "    #innitialize token list\n",
    "    haiku_tokens = []\n",
    "    other_tokens = []\n",
    "    \n",
    "    #remove punctuaiton from string \n",
    "    def remove_punc(text):\n",
    "\n",
    "        # initializing punctuations string\n",
    "        punc = '''!()-[]{};:'\"\\,,<>./?@#$%^&*_~'''\n",
    "\n",
    "        #remove punc if see one\n",
    "        for i in text:\n",
    "            if i in punc:\n",
    "                text = text.replace(i, '')\n",
    "\n",
    "        return text\n",
    "    \n",
    "    \n",
    "    #generate token list from poems \n",
    "    def token_list(text, remove, token_list):\n",
    "        for poem in text:\n",
    "            poem = remove(poem)\n",
    "            t = nltk.word_tokenize(poem.lower())\n",
    "\n",
    "            for j in t:\n",
    "                token_list.append(j)\n",
    "\n",
    "        return token_list\n",
    "\n",
    "    \n",
    "    token_list(haiku, remove_punc, haiku_tokens)\n",
    "    token_list(others, remove_punc, other_tokens)\n",
    "    \n",
    "    \n",
    "    #log dictionary \n",
    "    def logodds_with_uninformative_prior(one_tokens, two_tokens):\n",
    "    \n",
    "        def get_counter_from_list(tokens):\n",
    "            counter=Counter()\n",
    "            for token in tokens:\n",
    "                counter[token]+=1\n",
    "            return counter\n",
    "\n",
    "\n",
    "        oneCounter=get_counter_from_list(one_tokens)\n",
    "        twoCounter=get_counter_from_list(two_tokens)\n",
    "\n",
    "        vocab=dict(oneCounter) \n",
    "        vocab.update(dict(twoCounter))\n",
    "        oneSum=sum(oneCounter.values())\n",
    "        twoSum=sum(twoCounter.values())\n",
    "\n",
    "        ranks={}\n",
    "        alpha=0.01\n",
    "        alphaV=len(vocab)*alpha\n",
    "\n",
    "        for word in vocab:\n",
    "\n",
    "            log_odds_ratio=math.log( (oneCounter[word] + alpha) / (oneSum+alphaV-oneCounter[word]-alpha) ) - math.log( (twoCounter[word] + alpha) / (twoSum+alphaV-twoCounter[word]-alpha) )\n",
    "            variance=1./(oneCounter[word] + alpha) + 1./(twoCounter[word] + alpha)\n",
    "\n",
    "            ranks[word]=log_odds_ratio/math.sqrt(variance)\n",
    "\n",
    "        sorted_x = sorted(ranks.items(), key=operator.itemgetter(1), reverse=True)\n",
    "        \n",
    "        dictionary = {}\n",
    "        \n",
    "        for a, b in sorted_x:\n",
    "            dictionary.setdefault(a, []).append(b)\n",
    "        \n",
    "        return dictionary\n",
    "    \n",
    "    \n",
    "    \n",
    "    #getting feature based on log dictionary \n",
    "    def log_featurize(poem, feature_vocab, log_dic):\n",
    "        \n",
    "        # featurize text by just noting the binary presence of words within it\n",
    "        \n",
    "        feats={}\n",
    "        \n",
    "        poem = remove_punc(poem)\n",
    "        \n",
    "        tokens=nltk.word_tokenize(poem.lower())\n",
    "        \n",
    "        \n",
    "        for token in tokens:\n",
    "            \n",
    "            if token not in feature_vocab:\n",
    "                feature_vocab[token] = len(token)\n",
    "                \n",
    "            feats[feature_vocab[token]] = log_dic[token][0]\n",
    "            \n",
    "        return feats\n",
    "\n",
    "    feature_vocab={}\n",
    "    data=[]\n",
    "    Y=[]\n",
    "    \n",
    "    log_dic = logodds_with_uninformative_prior(haiku_tokens, other_tokens)\n",
    "\n",
    "    for poem in haiku:\n",
    "        #get the features\n",
    "        feats = log_featurize(poem, feature_vocab, log_dic)\n",
    "        \n",
    "        data.append(feats)\n",
    "        Y.append(1)\n",
    "        \n",
    "    for poem in others:\n",
    "        feats = log_featurize(poem, feature_vocab, log_dic)\n",
    "        data.append(feats)\n",
    "        Y.append(0)\n",
    "    \n",
    "    # since the data above has all haiku ordered before non-haiku, let's shuffle them\n",
    "    temp = list(zip(data, Y))\n",
    "    random.shuffle(temp)\n",
    "    data, Y = zip(*temp)\n",
    "\n",
    "    # we'll use a sparse representation since our features are sparse\n",
    "    X=sparse.lil_matrix((len(data), len(feature_vocab)))\n",
    "\n",
    "    for idx,feats in enumerate(data):\n",
    "        for f in feats:\n",
    "            X[idx,f]=feats[f]\n",
    "    \n",
    "    return X, Y, feature_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "19b47e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fancy_featurize_all_1(haiku, others):\n",
    "    \n",
    "    # your code here\n",
    "    \n",
    "        #remove punctuaiton from string \n",
    "    def remove_punc(text):\n",
    "\n",
    "        # initializing punctuations string\n",
    "        punc = '''!()-[]{};:'\"\\,,<>./?@#$%^&*_~'''\n",
    "\n",
    "        #remove punc if see one\n",
    "        for i in text:\n",
    "            if i in punc:\n",
    "                text = text.replace(i, '')\n",
    "\n",
    "        return text\n",
    "\n",
    "    def unigram_featurize(poem, feature_vocab):\n",
    "        \n",
    "        # featurize text by just noting the binary presence of words within it\n",
    "        \n",
    "        feats={}\n",
    "        tokens=nltk.word_tokenize(poem.lower())\n",
    "        \n",
    "        \n",
    "        for token in tokens:\n",
    "            \n",
    "            if token not in feature_vocab:\n",
    "                feature_vocab[token] = len(token)\n",
    "                \n",
    "            feats[feature_vocab[token]]=1\n",
    "            \n",
    "        return feats\n",
    "\n",
    "    feature_vocab={}\n",
    "    data=[]\n",
    "    Y=[]\n",
    "\n",
    "    for poem in haiku:\n",
    "        #get the features\n",
    "        poem = remove_punc(poem)\n",
    "        feats=unigram_featurize(poem, feature_vocab)\n",
    "        \n",
    "        data.append(feats)\n",
    "        Y.append(1)\n",
    "        \n",
    "    for poem in others:\n",
    "        poem = remove_punc(poem)\n",
    "        feats=unigram_featurize(poem, feature_vocab)\n",
    "        data.append(feats)\n",
    "        Y.append(0)\n",
    "    \n",
    "    # since the data above has all haiku ordered before non-haiku, let's shuffle them\n",
    "    temp = list(zip(data, Y))\n",
    "    random.shuffle(temp)\n",
    "    data, Y = zip(*temp)\n",
    "\n",
    "    # we'll use a sparse representation since our features are sparse\n",
    "    X=sparse.lil_matrix((len(data), len(feature_vocab)))\n",
    "\n",
    "    for idx,feats in enumerate(data):\n",
    "        for f in feats:\n",
    "            X[idx,f]=feats[f]\n",
    "    \n",
    "    return X, Y, feature_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "a06afb26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.106 NMI\n"
     ]
    }
   ],
   "source": [
    "run_all(haiku, others, fancy_featurize_all_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "02fe2547",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.014 NMI\n"
     ]
    }
   ],
   "source": [
    "run_all(haiku, others, fancy_featurize_log)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5feda2b4",
   "metadata": {},
   "source": [
    "**Q2**: Describe your method for featurization in 100 words and why you expect it to be able to separate haiku poems from non-haiku poems in this data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33951a56",
   "metadata": {},
   "source": [
    "1. Remove the punctuations in the source file. \n",
    "2. Using the token size in the featueset dictionary for each token, account for the token size instead of incrimenting length of the dictionary, the score got better around 0.014.\n",
    "3. Also tried to Use loggodds ratio to pick out the feature set for haiku tokens and the other's token, account for more appropriate sizing, however the score got way worse, around 0.014."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8095d5e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anlp] *",
   "language": "python",
   "name": "conda-env-anlp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
