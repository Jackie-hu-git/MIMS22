{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The log-odds ratio with an informative (and uninformative) Dirichlet prior (described in [Monroe et al. 2009, Fighting Words](http://languagelog.ldc.upenn.edu/myl/Monroe.pdf)) is a common method for finding distinctive terms in two datasets (see [Jurafsky et al. 2014](https://firstmonday.org/ojs/index.php/fm/article/view/4944/3863) for an example article that uses it to make an empirical argument). This method for finding distinguishing words combines a number of desirable properties:\n",
    "\n",
    "* it specifies an intuitive metric (the log-odds) for comparing word probabilities in two corpora\n",
    "* it incorporates prior information in the form of pseudocounts, which can either act as a smoothing factor (in the uninformative case) or incorporate real information about the expected frequency of words overall.\n",
    "* it accounts for variability of a frequency estimate by essentially converting the log-odds to a z-score.\n",
    "\n",
    "In this homework you will implement both of these ratios and compare the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, operator, math, nltk, itertools\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_and_tokenize(filename):\n",
    "    \n",
    "    with open(filename, encoding=\"utf-8\") as file:\n",
    "        tokens=[]\n",
    "        # lowercase\n",
    "        for line in file:\n",
    "            data=line.rstrip().lower()\n",
    "            # This dataset is already tokenized, so we can split on whitespace\n",
    "            tokens.extend(data.split(\" \"))\n",
    "        return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data we'll use in this case comes from a sample of 1000 positive and 1000 negative movie reviews from the [Large Movie Review Dataset](https://ai.stanford.edu/~amaas/data/sentiment/).  The version of the data used in this homework has already been tokenized for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_tokens=read_and_tokenize(\"../data/negative.reviews.txt\")\n",
    "positive_tokens=read_and_tokenize(\"../data/positive.reviews.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$n^i = $ number of words in corpus $i$ (likewise for $j$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "282868"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(positive_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "266021"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(negative_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$y_w^i = $ count of word $w$ in corpus $i$ (likewise for $j$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize Counters for both reviews and vocab dictionary \n",
    "pos_counts=Counter()\n",
    "neg_counts=Counter()\n",
    "    \n",
    "vocab={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#count distinct word count in each list \n",
    "for token in negative_tokens:\n",
    "    neg_counts[token]+=1\n",
    "    vocab[token]=1\n",
    "\n",
    "for token in positive_tokens:\n",
    "    pos_counts[token]+=1    \n",
    "    vocab[token]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20760"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pos_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19198"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(neg_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$V$ = size of vocabulary (number of distinct word types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29595"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#total of all distinct tokens in both list \n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1.  Implement the log-odds ratio with an uninformative Dirichlet prior.  This value, $\\hat\\zeta_w^{(i-j)}$ for word $w$ reflecting the difference in usage between corpus $i$ and corpus $j$, is given by the following equation:\n",
    "\n",
    "$$\n",
    "\\hat\\zeta_w^{(i-j)}= {\\hat{d}_w^{(i-j)} \\over \\sqrt{\\sigma^2\\left(\\hat{d}_w^{(i-j)}\\right)}}\n",
    "$$\n",
    "\n",
    "Where: \n",
    "\n",
    "$$\n",
    "\\hat{d}_w^{(i-j)} = \\log \\left({y_w^i + \\alpha_w} \\over {n^i + \\alpha_0 - y_w^i - \\alpha_w}) \\right) -  \\log \\left({y_w^j + \\alpha_w} \\over {n^j + \\alpha_0 - y_w^j - \\alpha_w}) \\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\sigma^2\\left(\\hat{d}_w^{(i-j)}\\right) \\approx {1 \\over {y_w^i + \\alpha_w}} + {1 \\over {y_w^j + \\alpha_w} }\n",
    "$$\n",
    "\n",
    "And:\n",
    "\n",
    "* $y_w^i = $ count of word $w$ in corpus $i$ (likewise for $j$)\n",
    "* $\\alpha_w$ = 0.01\n",
    "* $V$ = size of vocabulary (number of distinct word types)\n",
    "* $\\alpha_0 = V * \\alpha_w$\n",
    "* $n^i = $ number of words in corpus $i$ (likewise for $j$)\n",
    "\n",
    "Here the two corpora are the positive movie reviews (e.g., $i$ = positive) and the negative movie reviews (e.g., $j$ = negative). Using this metric, print out the 25 words most strongly aligned with the positive corpus, and 25 words most strongly aligned with the negative corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "#one_tokens -> neg ;  two_tokens -> pos\n",
    "def logodds_with_uninformative_prior(one_tokens, two_tokens, display=25):\n",
    "    \n",
    "    # complete this section\n",
    "    \n",
    "    #initialize Counters for both reviews and vocab dictionary \n",
    "    \n",
    "    neg_counts=Counter()\n",
    "    pos_counts=Counter()\n",
    "\n",
    "    vocab={}\n",
    "    \n",
    "    #count distinct word count in each list \n",
    "    for token in one_tokens:\n",
    "        neg_counts[token]+=1\n",
    "        vocab[token]=1\n",
    "\n",
    "    for token in two_tokens:\n",
    "        pos_counts[token]+=1    \n",
    "        vocab[token]=1\n",
    "    \n",
    "    #constant\n",
    "    a = 0.01\n",
    "    n_pos = len(positive_tokens)\n",
    "    n_neg = len(negative_tokens)\n",
    "    V = len(vocab)\n",
    "    \n",
    "    #numerator \n",
    "    for k, v in vocab.items():\n",
    "        v1 = (pos_counts[k] + a) / (n_pos + a * V - pos_counts[k] - a)\n",
    "        v2 = (neg_counts[k] + a) / (n_neg + a * V - neg_counts[k] - a)\n",
    "\n",
    "        p_pos = math.log(v1)\n",
    "        p_neg = math.log(v2)\n",
    "\n",
    "        n = p_pos - p_neg\n",
    "\n",
    "        #denomanator\n",
    "\n",
    "        sig = (1 / (pos_counts[k] + a)) + (1 / (neg_counts[k] + a))\n",
    "        d = math.sqrt (sig)\n",
    "\n",
    "        #output \n",
    "        o = n / d\n",
    "\n",
    "        #update the dictionary value to the new output \n",
    "        vocab[k] = o \n",
    "    \n",
    "    #sort dictionary\n",
    "    sorted_dict = sorted(vocab.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    #get the ranks\n",
    "    positiveTopWords = sorted_dict[:display]\n",
    "    negativeTopWords = sorted_dict[-display:][::-1]\n",
    "        \n",
    "    return positiveTopWords, negativeTopWords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([('great', 9.607540224421118),\n",
       "  ('his', 8.444585050679482),\n",
       "  ('best', 8.221173530354823),\n",
       "  ('as', 8.068276896558293),\n",
       "  ('and', 8.008455816960856),\n",
       "  ('love', 7.4664889052138665),\n",
       "  ('war', 7.231860280397061),\n",
       "  ('excellent', 7.142854492584097),\n",
       "  ('wonderful', 6.697252740688556),\n",
       "  ('is', 6.559130822066367),\n",
       "  ('her', 6.38883111352343),\n",
       "  ('performance', 6.052211880700309),\n",
       "  (',', 5.93698228878491),\n",
       "  ('of', 5.768768034806312),\n",
       "  ('life', 5.7217812496838585),\n",
       "  ('highly', 5.706899505669689),\n",
       "  ('world', 5.664462780541281),\n",
       "  ('perfect', 5.547139675935874),\n",
       "  ('in', 5.489693080913076),\n",
       "  ('always', 5.465521212926001),\n",
       "  ('performances', 5.380094126983327),\n",
       "  ('beautiful', 5.3556137393424645),\n",
       "  ('most', 5.198384926387417),\n",
       "  ('tony', 5.148053450218753),\n",
       "  ('loved', 5.0921802161916085)],\n",
       " [('bad', -15.874118374895222),\n",
       "  ('?', -15.035079097668289),\n",
       "  (\"n't\", -11.949393783552106),\n",
       "  ('movie', -10.959996673992299),\n",
       "  ('worst', -9.92867459130721),\n",
       "  ('i', -9.448181178355652),\n",
       "  ('just', -9.122270515824324),\n",
       "  ('...', -8.675997956464013),\n",
       "  ('was', -8.617584504048052),\n",
       "  ('no', -7.999249396143025),\n",
       "  ('do', -7.521169947814628),\n",
       "  ('awful', -7.511891984576927),\n",
       "  ('terrible', -7.446279268276979),\n",
       "  ('they', -7.372767849274727),\n",
       "  ('horrible', -7.052577619117426),\n",
       "  ('why', -7.019505671855516),\n",
       "  ('this', -6.934937867357723),\n",
       "  ('poor', -6.930684966472034),\n",
       "  ('boring', -6.709363182052385),\n",
       "  ('any', -6.684833313059192),\n",
       "  ('waste', -6.674077981733288),\n",
       "  ('script', -6.6612890317425215),\n",
       "  ('worse', -6.6012235452154595),\n",
       "  ('have', -6.55152365358799),\n",
       "  ('stupid', -6.4750566877612865)])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logodds_with_uninformative_prior(negative_tokens, positive_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2: As you increase the constant $\\alpha_w$ in the equations above, what happens to $\\hat\\zeta_w^{(i-j)}$, $\\hat{d}_w^{(i-j)}$ and $\\sigma^2\\left(\\hat{d}_w^{(i-j)}\\right)$ (i.e., do they get bigger or smaller)?  Answer this by plugging the following values in your implementation of these two quantities, and varying $\\alpha_w$ (and, consequently, $\\alpha_0$).\n",
    "\n",
    "* $y_w^i=34$\n",
    "* $y_w^j=17$\n",
    "* $n^i=1000$\n",
    "* $n^j=1000$\n",
    "* $V=500$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "$$\n",
    "\\hat{d}_w^{(i-j)} = \\log \\left({y_w^i + \\alpha_w} \\over {n^i + \\alpha_0 - y_w^i - \\alpha_w}) \\right) -  \\log \\left({y_w^j + \\alpha_w} \\over {n^j + \\alpha_0 - y_w^j - \\alpha_w}) \\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\sigma^2\\left(\\hat{d}_w^{(i-j)}\\right) \\approx {1 \\over {y_w^i + \\alpha_w}} + {1 \\over {y_w^j + \\alpha_w} }\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def explore_alpha(a):\n",
    "    \n",
    "    #constant\n",
    "    #a = 0.01\n",
    "    y_i = 34\n",
    "    y_j = 17\n",
    "    n_i = 1000\n",
    "    n_j = 1000\n",
    "    V = 500\n",
    "    \n",
    "    #numerator \n",
    "\n",
    "    v1 = (y_i + a) / (n_i + a * V - y_i - a)\n",
    "    v2 = (y_j + a) / (n_j + a * V - y_j - a)\n",
    "\n",
    "    p_pos = math.log(v1)\n",
    "    p_neg = math.log(v2)\n",
    "\n",
    "    n = p_pos - p_neg\n",
    "\n",
    "    #denomanator\n",
    "\n",
    "    sig = (1 / (y_i + a)) + (1 / (y_j + a))\n",
    "    d = math.sqrt (sig)\n",
    "\n",
    "    #output \n",
    "    o = n / d\n",
    "\n",
    "    return o, n, sig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2.3915077005224097, 0.7102095992733704, 0.08819206440820998)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_a = 0.01\n",
    "explore_alpha(original_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2.0092779913600722, 0.4912029668423381, 0.05976430976430976)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigger_a = 10\n",
    "explore_alpha(bigger_a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As alpha increase, these values decrease. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's make that prior informative by including information about the overall frequency of a given word in a background corpus (i.e., a corpus that represents general word usage, without regard for labeled subcorpora).  To do so, there are only two small changes to make:\n",
    "\n",
    "* We need to gather a background corpus $b$ and calculate $\\hat\\pi_w$, the relative frequency of word $w$ in $b$ (i.e., the number of times $w$ occurs in $b$ divided by the number of words in $b$).\n",
    "\n",
    "* In the uninformative prior above, $\\alpha_w$ was a constant (0.01) and $\\alpha_0 = V * \\alpha_w$.  Let us now set $\\alpha_0 = 1000$ and $\\alpha_w = \\hat\\pi_w * \\alpha_0$.  This reflects a pseudocount capturing the fractional number of times we would expect to see word $w$ in a sample of 1000 words.\n",
    "\n",
    "This allows us to specify that a common word like \"the\" (which has a relative frequency of $\\approx 0.04$) would have $\\alpha_w = 40$, while an infrequent word like \"beneficiaries\" (relative frequency $\\approx 0.00002$) would have $\\alpha_w = 0.02$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3. Implement a log-odds ratio with informative prior, using a larger background corpus of 5M tokens drawn from the same dataset (given to you as `priors` below, which contains the relative frequencies of words calculated from that corpus) and set $\\alpha_0 = 1000$. Using this metric, print out again the 25 words most strongly aligned with the positive corpus, and 25 words most strongly aligned with the negative corpus.  Is there a meaningful difference?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_priors(filename):\n",
    "    counts=Counter()\n",
    "    freqs={}\n",
    "    tokens=read_and_tokenize(filename)\n",
    "    total=len(tokens)\n",
    "\n",
    "    for token in tokens:\n",
    "        counts[token]+=1\n",
    "\n",
    "    for word in counts:\n",
    "        freqs[word]=counts[word]/total\n",
    "\n",
    "    return freqs\n",
    "    \n",
    "priors=read_priors(\"../data/sentiment.background.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logodds_with_informative_prior(one_tokens, two_tokens, priors, display=25):\n",
    "    \n",
    "    # complete this section\n",
    "     # complete this section\n",
    "    \n",
    "    #initialize Counters for both reviews and vocab dictionary \n",
    "    \n",
    "    neg_counts=Counter()\n",
    "    pos_counts=Counter()\n",
    "\n",
    "    vocab={}\n",
    "    \n",
    "    #count distinct word count in each list \n",
    "    for token in one_tokens:\n",
    "        neg_counts[token]+=1\n",
    "        vocab[token]=1\n",
    "\n",
    "    for token in two_tokens:\n",
    "        pos_counts[token]+=1    \n",
    "        vocab[token]=1\n",
    "    \n",
    "    #constant\n",
    "    a_0 = 1000\n",
    "    n_pos = len(positive_tokens)\n",
    "    n_neg = len(negative_tokens)\n",
    "    V = len(vocab)\n",
    "    \n",
    "    #numerator \n",
    "    for k, v in vocab.items():\n",
    "        v1 = (pos_counts[k] + a_0 * priors[k]) / (n_pos + a_0 - pos_counts[k] - a_0 * priors[k])\n",
    "        v2 = (neg_counts[k] + a_0 * priors[k]) / (n_neg + a_0 - neg_counts[k] - a_0 * priors[k])\n",
    "\n",
    "        p_pos = math.log(v1)\n",
    "        p_neg = math.log(v2)\n",
    "\n",
    "        n = p_pos - p_neg\n",
    "\n",
    "        #denomanator\n",
    "\n",
    "        sig = (1 / (pos_counts[k] + a_0 * priors[k])) + (1 / (neg_counts[k] + a_0 * priors[k]))\n",
    "        d = math.sqrt (sig)\n",
    "\n",
    "        #output \n",
    "        o = n / d\n",
    "\n",
    "        #update the dictionary value to the new output \n",
    "        vocab[k] = o \n",
    "    \n",
    "    #sort dictionary\n",
    "    sorted_dict = sorted(vocab.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    #get the ranks\n",
    "    positiveTopWords = sorted_dict[:display]\n",
    "    negativeTopWords = sorted_dict[-display:][::-1]\n",
    "        \n",
    "    return positiveTopWords, negativeTopWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([('great', 9.591039308150844),\n",
       "  ('his', 8.427876598766474),\n",
       "  ('best', 8.207390895133855),\n",
       "  ('as', 8.051850426312377),\n",
       "  ('and', 7.990231407305706),\n",
       "  ('love', 7.4541507138623375),\n",
       "  ('war', 7.2257038478692515),\n",
       "  ('excellent', 7.136307614907904),\n",
       "  ('wonderful', 6.692625533714691),\n",
       "  ('is', 6.543717647282005),\n",
       "  ('her', 6.3771329954200775),\n",
       "  ('performance', 6.042737335969907),\n",
       "  (',', 5.9215701353486665),\n",
       "  ('of', 5.754969742770618),\n",
       "  ('life', 5.712104514477569),\n",
       "  ('highly', 5.703588456479672),\n",
       "  ('world', 5.6554566648511475),\n",
       "  ('perfect', 5.540329074257345),\n",
       "  ('in', 5.477091362874526),\n",
       "  ('always', 5.456154882740344),\n",
       "  ('performances', 5.371497571484572),\n",
       "  ('beautiful', 5.346392148729829),\n",
       "  ('most', 5.188839322996821),\n",
       "  ('tony', 5.151557465549794),\n",
       "  ('loved', 5.08520207589855)],\n",
       " [('bad', -15.858436155657074),\n",
       "  ('?', -15.012171561204545),\n",
       "  (\"n't\", -11.92973245486743),\n",
       "  ('movie', -10.942156841842946),\n",
       "  ('worst', -9.958385464468778),\n",
       "  ('i', -9.43394122124469),\n",
       "  ('just', -9.10719606286112),\n",
       "  ('...', -8.661491103140472),\n",
       "  ('was', -8.60428995816332),\n",
       "  ('no', -7.98623150105582),\n",
       "  ('awful', -7.513361158186305),\n",
       "  ('do', -7.509005013285732),\n",
       "  ('terrible', -7.450145065620428),\n",
       "  ('they', -7.360873277110952),\n",
       "  ('horrible', -7.054947486972777),\n",
       "  ('why', -7.008333826955033),\n",
       "  ('this', -6.924931742171541),\n",
       "  ('poor', -6.923265303686221),\n",
       "  ('waste', -6.719492793687231),\n",
       "  ('boring', -6.70218207705369),\n",
       "  ('any', -6.674061366310463),\n",
       "  ('script', -6.651941912393764),\n",
       "  ('worse', -6.597130696108337),\n",
       "  ('have', -6.541331663788079),\n",
       "  ('stupid', -6.468495330825354)])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logodds_with_informative_prior(negative_tokens, positive_tokens, priors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's no differences in the words, but the value has changed. The words are the same, most likely because they have high frequencies. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
