{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Set 4\n",
    "\n",
    "### Before You Start\n",
    "\n",
    "This is a fun but challenging problem set. It will test your python skills, as well as your understanding of the material in class and in the readings. Start early and debug often! Some notes:\n",
    "\n",
    "* Part 1 is meant to be easy, so get through it quickly.\n",
    "* Part 2 (especially 2.1) will be difficult, but it is the lynchpin of this problem set so make sure to do it well and understand what you've done. If you find your gradient descent algorithm is taking more than a few minutes to complete, debug more, compare notes with others, and go to the TA sessions (especially the sections on vectorized computation and computational efficiency).\n",
    "* Depending on how well you've done 2.1, parts 2.3 and 3.3 will be relatively painless or incredibly painful. \n",
    "* Do the extra credit problems last!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Introduction to the assignment\n",
    "\n",
    "As with the last assignment, you will be using the Boston Housing Prices Data Set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import sklearn\n",
    "import random\n",
    "%matplotlib inline   \n",
    "import matplotlib.pyplot as plt  \n",
    "import statsmodels.api as sm\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import statsmodels.formula.api as smf\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Boston Housing data into a dataframe\n",
    "# Target.txt contains the median house values and Data.txt contains the other 13 features\n",
    "# in order [\"CRIM\", \"ZN\", \"INDUS\", \"CHAS\", \"NOX\",\"RM\",\"AGE\",\"DIS\",\"RAD\",\"TAX\",\"PTRATIO\",\"B\",\"LSTAT\"]\n",
    "data = np.loadtxt('data.txt')\n",
    "target = np.loadtxt('target.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Getting Oriented\n",
    "\n",
    "\n",
    "### 1.1 Use existing libraries\n",
    "\n",
    "Soon, you will write your own gradient descent algorithm, which you will then use to minimize the squared error cost function.  First, however, let's use the canned versions that come with Python, to make sure we understand what we're aiming to achieve.\n",
    "\n",
    "Using the same Boston housing prices dataset, explore the relationship between  median housing price and the percentage of lower status of the population using a linear regression model. Do this by - \n",
    "- (a) Regressing the `MEDV` (median housing price) on `LSTAT` (Percentage of lower status of the population)\n",
    "- (b) Regressing the `MEDV` (median housing price) on `LSTAT` **and** the natural log transformed value of `LSTAT`\n",
    "\n",
    "Interpret the results of both models by comparing them in terms of coefficients, their statistical significance and the variance explained by each model.\n",
    "\n",
    "\n",
    "*Hint-*\n",
    "* *You can use the [Linear Regression class](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html) from sklearn and OLS model from stats package.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First regression\n",
      "Coefficient: -0.95\n",
      "Intercept: 34.93\n",
      "R square: 0.54\n",
      "\n",
      "Second regression\n",
      "Coefficient of LSTAT: 0.43\n",
      "Coefficient of log of LSTAT: -18.11\n",
      "Intercept: 60.68\n",
      "R square: 0.67\n"
     ]
    }
   ],
   "source": [
    "# Your code here\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df.columns = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD','TAX', 'PTRATIO', 'B', 'LSTAT']\n",
    "df['MEDV'] = target\n",
    "\n",
    "regr_1 = LinearRegression()\n",
    "regr_1.fit(df[['LSTAT']], df['MEDV'])\n",
    "r_1 = regr_1.score(df[['LSTAT']], df['MEDV'])\n",
    "\n",
    "print('First regression')\n",
    "print('Coefficient: {:.2f}'.format(regr_1.coef_[0]))\n",
    "print('Intercept: {:.2f}'.format(regr_1.intercept_))\n",
    "print('R square: {:.2f}'.format(r_1))\n",
    "\n",
    "df_2 = df.copy()\n",
    "df_2['LSTAT_log'] = np.log(df_2['LSTAT'])\n",
    "\n",
    "regr_2 = LinearRegression()\n",
    "regr_2.fit(df_2[['LSTAT', 'LSTAT_log']], df_2['MEDV'])\n",
    "r_2 = regr_2.score(df_2[['LSTAT', 'LSTAT_log']], df_2['MEDV'])\n",
    "print('\\nSecond regression')\n",
    "print('Coefficient of LSTAT: {:.2f}'.format(regr_2.coef_[0]))\n",
    "print('Coefficient of log of LSTAT: {:.2f}'.format(regr_2.coef_[1]))\n",
    "print('Intercept: {:.2f}'.format(regr_2.intercept_))\n",
    "print('R square: {:.2f}'.format(r_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                   MEDV   R-squared:                       0.542\n",
      "Model:                            OLS   Adj. R-squared:                  0.541\n",
      "Method:                 Least Squares   F-statistic:                     597.1\n",
      "Date:                Tue, 16 Mar 2021   Prob (F-statistic):           1.42e-87\n",
      "Time:                        15:37:18   Log-Likelihood:                -1642.5\n",
      "No. Observations:                 506   AIC:                             3289.\n",
      "Df Residuals:                     504   BIC:                             3297.\n",
      "Df Model:                           1                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         34.9327      0.578     60.430      0.000      33.797      36.068\n",
      "LSTAT         -0.9488      0.039    -24.436      0.000      -1.025      -0.872\n",
      "==============================================================================\n",
      "Omnibus:                      134.617   Durbin-Watson:                   0.899\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              281.769\n",
      "Skew:                           1.436   Prob(JB):                     6.53e-62\n",
      "Kurtosis:                       5.262   Cond. No.                         31.2\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                   MEDV   R-squared:                       0.667\n",
      "Model:                            OLS   Adj. R-squared:                  0.666\n",
      "Method:                 Least Squares   F-statistic:                     504.2\n",
      "Date:                Tue, 16 Mar 2021   Prob (F-statistic):          6.71e-121\n",
      "Time:                        15:37:18   Log-Likelihood:                -1561.9\n",
      "No. Observations:                 506   AIC:                             3130.\n",
      "Df Residuals:                     503   BIC:                             3142.\n",
      "Df Model:                           2                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         60.6770      1.937     31.318      0.000      56.871      64.484\n",
      "LSTAT          0.4307      0.106      4.074      0.000       0.223       0.638\n",
      "LSTAT_log    -18.1144      1.318    -13.741      0.000     -20.704     -15.524\n",
      "==============================================================================\n",
      "Omnibus:                      111.176   Durbin-Watson:                   0.950\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              290.715\n",
      "Skew:                           1.084   Prob(JB):                     7.45e-64\n",
      "Kurtosis:                       6.015   Cond. No.                         149.\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "from statsmodels.stats.outliers_influence import summary_table\n",
    "x = df['LSTAT']\n",
    "y = df['MEDV']\n",
    "X = sm.add_constant(x)\n",
    "\n",
    "ols = sm.OLS(y, X).fit()\n",
    "print(ols.summary())\n",
    "\n",
    "\n",
    "x = df_2[['LSTAT', 'LSTAT_log']]\n",
    "y = df_2['MEDV']\n",
    "X = sm.add_constant(x)\n",
    "\n",
    "ols1 = sm.OLS(y, X).fit()\n",
    "print(ols1.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The univariate linear regression implies that, on average, median housing price decreases by $9500 for each percentage point increase in the fraction of the population that is of lower status. The second model implies that there is some curvature to this relationship -- see the figure below.  In both models, the coefficients are statistically significant. The latter model explains substantially more variation in median housing price than the former model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Regression lines\n",
    "\n",
    "Create a scatterplot of `MEDV` on the x-axis and `LSTAT` on the y-axis, and add the two regression lines from 1.1. Show the linear regression line in red, and the linear+log regression line (which may have curvature) in blue.  Label these two regression lines with the corresponding regression equations (i.e. the slope and intercept of the line).\n",
    "\n",
    "**Bonus**: Add the 95% confidence bands (i.e.  the area that has a 95% chance of containing the true regression line) to each of these lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from statsmodels.stats.outliers_influence import summary_table\n",
    "import statsmodels.api as sm\n",
    "df_2 = df_2.sort_values(by='LSTAT')\n",
    "x = df_2['LSTAT']\n",
    "y = df_2['MEDV']\n",
    "X = sm.add_constant(x)\n",
    "\n",
    "ols = sm.OLS(y, X).fit()\n",
    "st, dataa, ss2 = summary_table(ols, alpha=0.05)\n",
    "fittedvalues = dataa[:,2]\n",
    "predict_mean_se  = dataa[:,3]\n",
    "predict_mean_ci_low, predict_mean_ci_upp = dataa[:,4:6].T\n",
    "predict_ci_low, predict_ci_upp = dataa[:,6:8].T\n",
    "\n",
    "linear_eq = 'Y = {:.2f} + {:.2f}*x'.format(regr_1.intercept_, regr_1.coef_[0])\n",
    "quad_eq = 'Y = {:.2f} + {:.2f}*x + {:.2f}*log(x)'.format(regr_2.intercept_, regr_2.coef_[0], regr_2.coef_[1])\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8,6))\n",
    "ax.plot(x, y, 'o', c='g', markersize=5, markeredgecolor='none', alpha=0.5)\n",
    "ax.plot(X, fittedvalues, 'r-', label='OLS')\n",
    "ax.plot(X, predict_mean_ci_low, 'r--')\n",
    "ax.plot(X, predict_mean_ci_upp, 'r--')\n",
    "\n",
    "x = df_2[['LSTAT', 'LSTAT_log']]\n",
    "y = df_2['MEDV']\n",
    "X = sm.add_constant(x)\n",
    "\n",
    "ols = sm.OLS(y, X).fit()\n",
    "\n",
    "st, dataaa, ss2 = summary_table(ols, alpha=0.05)\n",
    "fittedvalues = dataaa[:,2]\n",
    "predict_mean_se  = dataaa[:,3]\n",
    "predict_mean_ci_low, predict_mean_ci_upp = dataaa[:,4:6].T\n",
    "predict_ci_low, predict_ci_upp = dataaa[:,6:8].T\n",
    "\n",
    "#print(type(X['RM']))\n",
    "ax.plot(X['LSTAT'], fittedvalues, 'b-', label='OLS')\n",
    "ax.plot(X['LSTAT'], predict_mean_ci_low, 'b--')\n",
    "ax.plot(X['LSTAT'], predict_mean_ci_upp, 'b--')\n",
    "plt.xlabel('Percentage of lower status of the population')\n",
    "plt.ylabel('Median value of owner-occupied homes ($1000)')\n",
    "ax.set_xlim(3,9)\n",
    "ax.set_ylim(0,60)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.yaxis.set_ticks_position('left')\n",
    "ax.xaxis.set_ticks_position('bottom')\n",
    "ax.text(4.5, 2, r'$' + linear_eq + '$', fontsize=15, color='r')\n",
    "ax.text(5.1, 52, r'$' + quad_eq + '$', fontsize=15, color='b')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Cross-Validation\n",
    "\n",
    "Chances are, for the above problem you used all of your data to fit the regression line. In some circumstances this is a reasonable thing to do, but often this will result in overfitting. Let's redo the above results the ML way, using careful cross-validation.  Since you are now experts in cross-validation, and have written your own cross-validation algorithm from scratch, you can now take a shortcut and use the libraries that others have built for you.\n",
    "\n",
    "Before you proceed with cross-validation, split your dataset into training and test sets, using a 90-10 split. You can use the [train-test split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) functions from sklearn. After that, using the [cross-validation functions](http://scikit-learn.org/stable/modules/cross_validation.html) from scikit-learn, use 40-fold cross-validation to fit regression (a) from above, i.e. the linear fit of `MEDV` on `LSTAT`. Remember to use the **training set only**. We'll touch the test set only later. Each fold of cross-validation will give you one slope coefficient and one intercept coefficient.  Plot the distribution of the 40 slope coefficients using a histogram, then draw a vertical line at the value of the slope coefficient that you estimated in 1.1 using the full dataset.  What do you notice?\n",
    "\n",
    "\n",
    "*NOTE - In the `train_test_split` function be sure to set a random_state of 42.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# your code here\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data, target, test_size=0.1, random_state=42)\n",
    "\n",
    "df_train = pd.DataFrame(X_train)\n",
    "df_train.columns = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD','TAX', 'PTRATIO', 'B', 'LSTAT']\n",
    "df_train['MEDV'] = y_train\n",
    "\n",
    "df_test = pd.DataFrame(X_test)\n",
    "df_test.columns = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD','TAX', 'PTRATIO', 'B', 'LSTAT']\n",
    "df_test['MEDV'] = y_test\n",
    "\n",
    "\n",
    "kf = KFold(n_splits=40)\n",
    "slope = []\n",
    "for train_index, test_index in kf.split(df_train):\n",
    "    regr = LinearRegression()\n",
    "    dff = df_train.iloc[train_index]\n",
    "    regr.fit(dff[['LSTAT']], dff['MEDV'])\n",
    "    slope.append(regr.coef_[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.lines.Line2D at 0x14a7ef759b0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ax = plt.subplot(111)\n",
    "ax.hist(slope,color='g')\n",
    "plt.xlabel('Slope')\n",
    "plt.ylabel('Number')\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.yaxis.set_ticks_position('left')\n",
    "ax.xaxis.set_ticks_position('bottom')\n",
    "ax.axvline(regr_1.coef_[0], color='k', linestyle='--')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.967325504469232,\n",
       " -0.9352900735859123,\n",
       " -0.9535173915855782,\n",
       " 0.008082196302153037)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.min(slope), np.max(slope), np.mean(slope), np.std(slope)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distribution of slope coefficients estimated with k-fold cross validation range from -0.96 to -0.93, with an average value of -0.95 and standard deviation of 0.008.  Our previous estimate of the slope from 1.1 falls in the middle of this distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Gradient Descent: Linear Regression\n",
    "\n",
    "This is where it gets fun!\n",
    "\n",
    "*Note - For all of part 2, only use your training set that you created in 1.3*\n",
    "\n",
    "### 2.1 Implement gradient descent with one independent variable (LSTAT)\n",
    "\n",
    "Implement the batch gradient descent algorithm that we discussed in class. Use the version you implement to regress the housing price on the percentage of lower status of the population i.e. `MEDV`~`LSTAT`. Experiment with 3-4 different values of the learning rate *R*, and do the following for each *R*:\n",
    "\n",
    "* Report the values of alpha and beta that minimize the loss function\n",
    "* Report the number of iterations it takes for your algorithm to converge\n",
    "* Report the total running time of your algorithm, in seconds\n",
    "\n",
    "How do your coefficients compare to the ones estimated through standard libraries? Does this depend on *R*?\n",
    "\n",
    "Some skeleton code is provided below, but you should feel free to delete this code and start from scratch if you prefer.\n",
    "\n",
    "\n",
    "*HINTS-*\n",
    "* Don't forget to implement a stopping condition, so that at every iteration you check whether your results have converged. Common approaches to this are to - \n",
    "    * (a) check to see if the loss has stopped decreasing,\n",
    "    * (b) check if both your current parameter estimates are close to the estimates from the previous iteration.  In both cases, \"close\" should not be ==0, it should be <=epsilon, where epsilon is something very small (like 1e-6).\n",
    "* We recommend you include a MaxIterations parameter in your gradient descent algorithm, to prevent divergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OLS  model parameters\n",
      " intercept: 34.93, coefficient: -0.95\n",
      "\n",
      "Test 1\n",
      "Did not converge / Diverged\n",
      "Time taken: 0.00 seconds\n",
      "(-27.38434879586854, -490.0999983233098)\n",
      "\n",
      "Test 2\n",
      "Did not converge / Diverged\n",
      "Time taken: 0.00 seconds\n",
      "(0.13465761094241352, -0.49410142883658903)\n",
      "\n",
      "Test 3\n",
      "cost < 0.0001 stopped after iteration times: 38532\n",
      "Time taken: 0.76 seconds\n",
      "(35.03451684415056, -0.9532408619349316)\n",
      "\n",
      "Test 4\n",
      "cost < 0.0001 stopped after iteration times: 286862\n",
      "Time taken: 8.97 seconds\n",
      "(34.9960191900755, -0.950962415760394)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "\"\"\"\n",
    "Function\n",
    "--------\n",
    "bivariate_ols\n",
    "    Gradient Descent to minimize OLS. Used to find co-efficients of bivariate OLS Linear regression\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "xvalues, yvalues : narray\n",
    "    xvalues: independent variable\n",
    "    yvalues: dependent variable\n",
    "    \n",
    "R: float\n",
    "    Learning rate\n",
    "    \n",
    "MaxIterations: Int\n",
    "    maximum number of iterations\n",
    "    \n",
    "\n",
    "Returns\n",
    "-------\n",
    "alpha: float\n",
    "    intercept\n",
    "    \n",
    "beta: float\n",
    "    coefficient\n",
    "\"\"\"\n",
    "def bivariate_ols(xvalues, yvalues, R=0.01, MaxIterations=1000):\n",
    "    start_time = time.time()\n",
    "    alpha = 0.0\n",
    "    beta = 0.0\n",
    "    n = len(xvalues)\n",
    "    cost = np.inf\n",
    "    epsilon = 1e-6\n",
    "    xvaluest = np.array(xvalues)\n",
    "    yvaluest = np.array(yvalues)\n",
    "    for i in range(MaxIterations):\n",
    "        alpha_old = alpha\n",
    "        beta_old = beta\n",
    "        cost_old = cost\n",
    "        #IPython.embed()\n",
    "#         idx = np.random.choice(len(xvaluesg), 30, replace  =False)\n",
    "#         xvaluest=xvaluesg[idx]\n",
    "#         yvaluest=yvaluesg[idx]\n",
    "        #IPython.embed()\n",
    "        #print(i)\n",
    "        alpha -= R / n * ((alpha + beta * xvaluest - yvaluest).sum())\n",
    "        beta -= R / n * (((alpha_old + beta * xvaluest - yvaluest) * xvaluest).sum())\n",
    "        cost = 1.0 / (2 * n) * (((alpha + beta * xvaluest - yvaluest) ** 2).sum())\n",
    "        if cost_old - cost < 0:\n",
    "            print('Did not converge / Diverged')\n",
    "            break\n",
    "        if abs(alpha - alpha_old) < epsilon and abs(beta - beta_old) < epsilon:\n",
    "            print('cost < 0.0001 stopped after iteration times: {}'.format(i))\n",
    "            break\n",
    "    print(\"Time taken: {:.2f} seconds\".format(time.time() - start_time))\n",
    "    return alpha, beta\n",
    "\n",
    "\n",
    "print(\"OLS  model parameters\\n intercept: {:.2f}, coefficient: {:.2f}\".format(regr_1.intercept_, regr_1.coef_[0]))\n",
    "\n",
    "print('\\nTest 1')\n",
    "print(bivariate_ols(df_train['LSTAT'], df_train['MEDV'], 0.1, 1000000))\n",
    "\n",
    "print('\\nTest 2')\n",
    "print(bivariate_ols(df_train['LSTAT'], df_train['MEDV'], 0.01, 1000000))\n",
    "\n",
    "print('\\nTest 3')\n",
    "print(bivariate_ols(df_train['LSTAT'], df_train['MEDV'], 0.001, 1000000))\n",
    "\n",
    "print('\\nTest 4')\n",
    "print(bivariate_ols(df_train['LSTAT'], df_train['MEDV'], 0.0001, 1000000))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient descent failed to converge with a learning rate of 0.1 and 0.01. However, with smaller learning rates, gradient descent converged to parameter values very similar to the coefficients computed via the OLS packages. Smaller learning rates require more iterations and thus take longer to complete."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Data normalization (done for you!)\n",
    "\n",
    "Soon, you will implement a version of gradient descent that can use an arbitrary number of independent variables. Before doing this, we want to give you some code in case you want to standardize your features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize(raw_data):\n",
    "    return ((raw_data - np.mean(raw_data, axis = 0)) / np.std(raw_data, axis = 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Implement gradient descent with an arbitrary number of independent variables\n",
    "\n",
    "Now that you have a simple version of gradient descent working, create a version of gradient descent that can take more than one independent variable.  Assume all independent variables will be continuous.  Test your algorithm using `CRIM` and `LSTAT` as independent variables. **Standardize the variables before inputting them to the gradient descent algorithm**. \n",
    "\n",
    "As before,  report and interpret your estimated coefficients, the number of iterations before convergence, and the total running time of your algorithm. Experiment with 2-3 different values of R.\n",
    "\n",
    "*Hint: Be careful to implement this efficiently, otherwise it might take a long time for your code to run. Commands like `np.dot` can be a good friend to you on this problem.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function\n",
    "--------\n",
    "multivariate_ols\n",
    "    Gradient Descent to minimize OLS. Used to find coefficients of bivariate OLS Linear regression\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "xvalue_matrix, yvalues : narray\n",
    "    xvalue_matrix: independent variable\n",
    "    yvalues: dependent variable\n",
    "    \n",
    "R: float\n",
    "    Learning rate\n",
    "    \n",
    "MaxIterations: Int\n",
    "    maximum number of iterations\n",
    "    \n",
    "\n",
    "Returns\n",
    "-------\n",
    "alpha: float\n",
    "    intercept\n",
    "    \n",
    "beta_array: array[float]\n",
    "    co-efficient\n",
    "\"\"\"\n",
    "\n",
    "def multivariate_ols(xvalue_matrix, yvalues, R=0.01, MaxIterations=1000):\n",
    "    start_time = time.time()\n",
    "    xvalue_matrix = np.array(xvalue_matrix)\n",
    "    yvalues = np.array(yvalues)[:, np.newaxis]\n",
    "    n, variable_n = xvalue_matrix.shape # sample number, varaibels number\n",
    "    alpha = 0.0\n",
    "    beta_array = np.transpose(np.array([[0.0] * variable_n]))\n",
    "    cost = np.inf\n",
    "    epsilon = 1e-6\n",
    "    for i in range(MaxIterations):\n",
    "        cost_old = cost\n",
    "        alpha_old = alpha\n",
    "        beta_array_old = np.copy(beta_array)\n",
    "        # part_1: alpha + beta*x - y\n",
    "        part_1 = alpha_old + np.dot(xvalue_matrix, beta_array_old) - yvalues\n",
    "        alpha -= R / n * (part_1.sum())\n",
    "        beta_array -= R / n * (np.dot(xvalue_matrix.T, part_1))\n",
    "        part_2 = alpha + np.dot(xvalue_matrix, beta_array) - yvalues\n",
    "        cost = 1.0 / (2 * n) * ((part_2 ** 2).sum())\n",
    "        if cost_old - cost < 0:\n",
    "            print('not converge stopped after iteration times: {}'.format(i))\n",
    "            break\n",
    "        if abs(alpha-alpha_old) < epsilon and max(abs(beta_array-beta_array_old)) < epsilon:\n",
    "            print('cost < 0.00001 stopped after iteration times: {}'.format(i))\n",
    "            break\n",
    "    print(\"Time taken: {:.2f} seconds\".format(time.time() - start_time))\n",
    "    return alpha, beta_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 1\n",
      "cost < 0.00001 stopped after iteration times: 209\n",
      "Time taken: 0.01 seconds\n",
      "(22.694505488922747, array([[-0.71632478],\n",
      "       [-6.51928474]]))\n",
      "\n",
      "Test 2\n",
      "cost < 0.00001 stopped after iteration times: 1726\n",
      "Time taken: 0.04 seconds\n",
      "(22.69450483699636, array([[-0.71648559],\n",
      "       [-6.51912393]]))\n",
      "\n",
      "Test 3\n",
      "cost < 0.00001 stopped after iteration times: 13190\n",
      "Time taken: 0.33 seconds\n",
      "(22.694463395104854, array([[-0.71809157],\n",
      "       [-6.51751791]]))\n"
     ]
    }
   ],
   "source": [
    "df_cp = df_train.copy()\n",
    "df_norm = standardize(df_cp)\n",
    "df_norm['MEDV'] = df_train['MEDV']\n",
    "print('Test 1')\n",
    "print(multivariate_ols(df_norm[['CRIM', 'LSTAT']], df_norm['MEDV'], 0.1, 1000000))\n",
    "\n",
    "print('\\nTest 2')\n",
    "print(multivariate_ols(df_norm[['CRIM', 'LSTAT']], df_norm['MEDV'], 0.01, 1000000))\n",
    "\n",
    "print('\\nTest 3')\n",
    "print(multivariate_ols(df_norm[['CRIM', 'LSTAT']], df_norm['MEDV'], 0.001, 1000000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intercept: 22.69\n",
      "Coefficient of CRIM: -0.72\n",
      "Coefficient of LSTAT: -6.52\n"
     ]
    }
   ],
   "source": [
    "regr_3 = LinearRegression()\n",
    "regr_3.fit(df_norm[['CRIM', 'LSTAT']], df_norm['MEDV'])\n",
    "print('Intercept: {:.2f}'.format(regr_3.intercept_))\n",
    "print('Coefficient of CRIM: {:.2f}'.format(regr_3.coef_[0]))\n",
    "print('Coefficient of LSTAT: {:.2f}'.format(regr_3.coef_[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the features are standardized, the algorithm converges quickly. However, with standardized features, it is harder to literally interpret the coefficients, as they relate to a 1-SD increase in our independent variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Compare standardized vs. non-standardized results\n",
    "\n",
    "Repeat the analysis from 2.3, but this time do not standardize your variables - i.e., use the original training data. Use these three values of R - (0.1, 0.01, and 0.001). What do you notice about the running time and convergence properties of your algorithm?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 1\n",
      "not converge stopped after iteration times: 1\n",
      "Time taken: 0.00 seconds\n",
      "(-29.557719540017057, array([[-230.46156989],\n",
      "       [-533.71780415]]))\n",
      "\n",
      "Test 2\n",
      "not converge stopped after iteration times: 1\n",
      "Time taken: 0.00 seconds\n",
      "(0.11292390350092829, array([[-1.29079271],\n",
      "       [-0.93027949]]))\n",
      "\n",
      "Test 3\n",
      "cost < 0.00001 stopped after iteration times: 39748\n",
      "Time taken: 0.95 seconds\n",
      "(34.782473821363986, array([[-0.08215959],\n",
      "       [-0.90928457]]))\n"
     ]
    }
   ],
   "source": [
    "print('Test 1')\n",
    "print(multivariate_ols(df_train[['CRIM', 'LSTAT']], df_train['MEDV'], 0.1, int(1e6)))\n",
    "\n",
    "print('\\nTest 2')\n",
    "print(multivariate_ols(df_train[['CRIM', 'LSTAT']], df_train['MEDV'], 0.01, int(1e6)))\n",
    "\n",
    "print('\\nTest 3')\n",
    "print(multivariate_ols(df_train[['CRIM', 'LSTAT']], df_train['MEDV'], 0.001, int(1e6)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intercept: 34.787\n",
      "Coefficient of CRIM: -0.082\n",
      "Coefficient of LSTAT: -0.910\n"
     ]
    }
   ],
   "source": [
    "regr_4 = LinearRegression()\n",
    "regr_4.fit(df_train[['CRIM', 'LSTAT']], df_train['MEDV'])\n",
    "print('Intercept: {:.3f}'.format(regr_4.intercept_))\n",
    "print('Coefficient of CRIM: {:.3f}'.format(regr_4.coef_[0]))\n",
    "print('Coefficient of LSTAT: {:.3f}'.format(regr_4.coef_[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we do not standardize our features, our algorithm does not always converge, or it takes a very long time to do so. However, for an appropriate learning rate (e.g., R=0.001), we observe convergence to the same coefficients estimated through standard statistical packages (such as sklearn). In this case, the relationship indicates that an increase in the crime rate of one unit is associated with a decrease in housing prices of \\\\$82, while controlling for LSTAT ; and a one-unit increase in the percentage of lower status of the population is associated with a decrease in prices of \\\\$910, while controlling for CRIM. Note that when controlling for CRIM, the relationship between LSTAT and MEDV is different than it was in the 1.1. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Extra Credit 1 :  Implement  Mini-Batch Gradient Descent (MB-GD)\n",
    "MB-GD is a Gradient Descent variant that in large data sets can converge faster and is computationally less intensive. Implement MB-GD for question 2.3 (ensure you're using standardized data). Experiment with different values for the learning rate, number of iterations and \"mini-batch\" size\n",
    "so that you compute the estimates within a 1e-3 tolerance. Do not use a batch-size greater than 32.\n",
    "MB-GD is similar to Stochastic Gradient Descent but instead of using one sample to compute the gradient we use a batch of samples at each iteration. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "\"\"\"\n",
    "Function\n",
    "--------\n",
    "multivariate_ols\n",
    "    Gradient Decent to minimize OLS. Used to find co-efficients of bivariate OLS Linear regression\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "xvalue_matrix, yvalues : narray\n",
    "    xvalue_matrix: independent variable\n",
    "    yvalues: dependent variable\n",
    "    \n",
    "R: float\n",
    "    Learning rate\n",
    "    \n",
    "MaxIterations: Int\n",
    "    maximum number of iterations\n",
    "    \n",
    "Batch_Size : Int\n",
    "    batch size of mini-batches\n",
    "\n",
    "Returns\n",
    "-------\n",
    "alpha: float\n",
    "    intercept\n",
    "    \n",
    "beta_array: array[float]\n",
    "    co-efficient\n",
    "\"\"\"\n",
    "\n",
    "def MBGD_multivariate_ols(xvalue_matrix, yvalues, R, MaxIterations, BATCH_SIZE):\n",
    "    start_time = time.time()\n",
    "    xvalue_matrix = np.array(xvalue_matrix)\n",
    "    yvalues = np.array(yvalues)[:, np.newaxis]\n",
    "    N, variable_n = xvalue_matrix.shape \n",
    "    alpha = 0.0\n",
    "    beta_array = np.transpose(np.array([[0.0] * variable_n]))\n",
    "    cost = np.inf\n",
    "    epsilon = 1e-6\n",
    "    cost_l = []\n",
    "    for i in range(MaxIterations):\n",
    "        cost_old = cost\n",
    "        alpha_old = alpha\n",
    "        beta_array_old = np.copy(beta_array) \n",
    "        X, y = shuffle(xvalue_matrix, yvalues)\n",
    "        for offset in range(0, N, BATCH_SIZE):\n",
    "            end = offset + BATCH_SIZE\n",
    "            batch_x, batch_y = X[offset:end], y[offset:end]\n",
    "            part_1 = alpha_old + np.dot(batch_x, beta_array_old) - batch_y \n",
    "            n = BATCH_SIZE\n",
    "            alpha -= R / n * (part_1.sum())\n",
    "            beta_array -= R / n * (np.dot(batch_x.T, part_1))\n",
    "            part_2 = alpha + np.dot(batch_x, beta_array) - batch_y \n",
    "            cost = 1.0 / (2 * n) * ((part_2 ** 2).sum())\n",
    "            cost_l.append(cost)\n",
    "#         if cost_old - cost < 0:\n",
    "#             print('not converge stopped after iteration times: {}'.format(i))\n",
    "#             break\n",
    "        if abs(alpha-alpha_old) < epsilon and max(abs(beta_array-beta_array_old)) < epsilon:\n",
    "            print('cost < 0.00001 stopped after iteration times: {}'.format(i))\n",
    "            break\n",
    "    print(\"Time taken: {:.2f} seconds\".format(time.time() - start_time))\n",
    "    return alpha, beta_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MBGD Test 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\qkava\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:47: RuntimeWarning: overflow encountered in reduce\n",
      "  return umr_sum(a, axis, dtype, out, keepdims, initial, where)\n",
      "C:\\Users\\qkava\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:56: RuntimeWarning: overflow encountered in square\n",
      "C:\\Users\\qkava\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:47: RuntimeWarning: invalid value encountered in reduce\n",
      "  return umr_sum(a, axis, dtype, out, keepdims, initial, where)\n",
      "C:\\Users\\qkava\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:54: RuntimeWarning: invalid value encountered in subtract\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 57.14 seconds\n",
      "(nan, array([[nan],\n",
      "       [nan]]))\n",
      "MBGD Test 2\n",
      "Time taken: 32.56 seconds\n",
      "(nan, array([[nan],\n",
      "       [nan]]))\n",
      "MBGD Test 3\n",
      "cost < 0.00001 stopped after iteration times: 76\n",
      "Time taken: 0.04 seconds\n",
      "(22.694505494358694, array([[-0.71631338],\n",
      "       [-6.51929614]]))\n",
      "MBGD Test 4\n",
      "cost < 0.00001 stopped after iteration times: 149\n",
      "Time taken: 0.05 seconds\n",
      "(22.694505492189293, array([[-0.71632013],\n",
      "       [-6.51928938]]))\n",
      "MBGD Test 5\n",
      "cost < 0.00001 stopped after iteration times: 669\n",
      "Time taken: 0.39 seconds\n",
      "(22.694505403026998, array([[-0.71637019],\n",
      "       [-6.51923933]]))\n",
      "MBGD Test 6\n",
      "cost < 0.00001 stopped after iteration times: 1256\n",
      "Time taken: 0.41 seconds\n",
      "(22.694505149301058, array([[-0.71643329],\n",
      "       [-6.51917623]]))\n"
     ]
    }
   ],
   "source": [
    "df_cp = df_train.copy()\n",
    "df_norm = standardize(df_cp)\n",
    "df_norm['MEDV'] = df_train['MEDV']\n",
    "\n",
    "print('MBGD Test 1')\n",
    "print(MBGD_multivariate_ols(df_norm[['CRIM', 'LSTAT']], df_norm['MEDV'], 0.1, int(1e5), 16))\n",
    "\n",
    "print('MBGD Test 2')\n",
    "print(MBGD_multivariate_ols(df_norm[['CRIM', 'LSTAT']], df_norm['MEDV'], 0.1, int(1e5), 32))\n",
    "\n",
    "print('MBGD Test 3')\n",
    "print(MBGD_multivariate_ols(df_norm[['CRIM', 'LSTAT']], df_norm['MEDV'], 0.01, int(1e5), 16))\n",
    "\n",
    "print('MBGD Test 4')\n",
    "print(MBGD_multivariate_ols(df_norm[['CRIM', 'LSTAT']], df_norm['MEDV'], 0.01, int(1e5), 32))\n",
    "\n",
    "print('MBGD Test 5')\n",
    "print(MBGD_multivariate_ols(df_norm[['CRIM', 'LSTAT']], df_norm['MEDV'], 0.001, int(1e5), 16))\n",
    "\n",
    "print('MBGD Test 6')\n",
    "print(MBGD_multivariate_ols(df_norm[['CRIM', 'LSTAT']], df_norm['MEDV'], 0.001, int(1e5), 32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Extra Credit 2: Logistic Regression\n",
    "\n",
    "For extra credit, implement logistic regression using gradient descent. Create a new variable (EXPENSIVE) to indicate whether the median housing price is more than $40,000. Use your model to estimate a logistic regression of EXPENSIVE on CHAS and RM. Report your results and interpret the coefficients (i.e., explain what they imply in a sentence that your parents would understand)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost < 0.00001 stopped after iteration times: 2691\n",
      "Time taken: 0.18 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(-4.200024497328655,\n",
       " array([[0.32822603],\n",
       "        [1.95170989]]))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_norm['EXPENSIVE'] = df_norm.apply(lambda x: 1 if x['MEDV']>40 else 0, axis=1)\n",
    "\n",
    "def multivariate_lg(xvalue_matrix, yvalues, R=0.01, MaxIterations=1000):\n",
    "    start_time = time.time()\n",
    "    xvalue_matrix = np.array(xvalue_matrix)\n",
    "    yvalues = np.array(yvalues)[:, np.newaxis]\n",
    "    n, variable_n = xvalue_matrix.shape # sample number, varaibels number\n",
    "    alpha = 0.0\n",
    "    beta_array = np.transpose(np.array([[0.0] * variable_n]))\n",
    "    cost = np.inf\n",
    "    epsilon = 0.0001\n",
    "    for i in range(MaxIterations):\n",
    "        cost_old = cost\n",
    "        alpha_old = alpha\n",
    "        beta_array_old = np.copy(beta_array)\n",
    "        part_1 = 1.0 /(1 + np.exp(-(alpha_old + np.dot(xvalue_matrix, beta_array_old)))) - yvalues\n",
    "        alpha -= R / n * (part_1.sum())\n",
    "        beta_array -= R / n * (np.dot(np.transpose(xvalue_matrix), part_1))\n",
    "        part_2 = 1.0 /(1 + np.exp(-(alpha + np.dot(xvalue_matrix, beta_array)))) - yvalues\n",
    "        y_predict = 1.0 /(1 + np.exp(-(alpha + np.dot(xvalue_matrix, beta_array))))\n",
    "        cost = np.sum(- yvalues * np.log(y_predict) - (1 - yvalues) * np.log(1 - y_predict))\n",
    "        if cost_old - cost < 0:\n",
    "            print('not converge stopped after iteration times: {}'.format(i))\n",
    "            break\n",
    "        if abs(alpha-alpha_old) < epsilon and max(abs(beta_array-beta_array_old)) < epsilon:\n",
    "            print('cost < 0.00001 stopped after iteration times: {}'.format(i))\n",
    "            break\n",
    "    print(\"Time taken: {:.2f} seconds\".format(time.time() - start_time))\n",
    "    return alpha, beta_array\n",
    "\n",
    "multivariate_lg(df_norm[['CHAS','RM']], df_norm['EXPENSIVE'], 0.1, 100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-4.13442909]\n",
      "[[0.32305322 1.87523552]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = LogisticRegression().fit(df_norm[['CHAS','RM']], df_norm['EXPENSIVE'])\n",
    "print(clf.intercept_)\n",
    "print(clf.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our logistic regression results imply that the log odds ratio of being expensive increases by 0.32 for houses near the Charles river, and by 1.95 for each additional room. In other words, the probability of being expensive is ZZZ% higher for houses near the Charles river, and AAAA% higher for each additional room."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Regularization \n",
    "\n",
    "### 3.1 Get prepped\n",
    "\n",
    "Using the non-standardized train and test sets you created in 1.3, for each set create new interaction variables between each possible pair of the features. If you originally had *K* features, you should now have K+(K*(K+1))/2 features. Now standardize all of your features. For expediency, it is okay if you standardize the training and test sets separately, i.e., you do not need to use the mean and standard deviation of the training set to also standardize the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "bdfc = df_train.iloc[:,:-1].copy()\n",
    "\n",
    "cols = list(bdfc.columns.values)\n",
    "cols_n = len(cols)\n",
    "for i in range(cols_n):\n",
    "    for j in range(i+1):\n",
    "        c_name = cols[i] + '_' + cols[j]\n",
    "        bdfc[c_name] = bdfc[cols[i]] * bdfc[cols[j]]\n",
    "\n",
    "\n",
    "# poly = PolynomialFeatures(interaction_only=True, include_bias=False)\n",
    "# bdfcc = poly.fit_transform(bdfc)\n",
    "bdf_tr=pd.DataFrame(standardize(bdfc))\n",
    "bdf_tr['MEDV'] = df_train['MEDV']\n",
    "\n",
    "\n",
    "#Create test set's features\n",
    "bdfc1 = df_test.iloc[:,:-1].copy()\n",
    "\n",
    "cols = list(bdfc1.columns.values)\n",
    "cols_n = len(cols)\n",
    "for i in range(cols_n):\n",
    "    for j in range(i + 1):\n",
    "        c_name = cols[i] + '_' + cols[j]\n",
    "        bdfc1[c_name] = bdfc1[cols[i]] * bdfc1[cols[j]]\n",
    "\n",
    "# poly = PolynomialFeatures(interaction_only=True, include_bias=False)\n",
    "# bdfcc1 = poly.fit_transform(bdfc1)\n",
    "\n",
    "bdf_test=pd.DataFrame(standardize(bdfc1))\n",
    "bdf_test['MEDV'] = df_test['MEDV']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((455, 105), (455,))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bdf_tr.shape, bdf_tr.MEDV.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD',\n",
       "       'TAX', 'PTRATIO', 'B', 'LSTAT', 'CRIM_CRIM', 'ZN_CRIM', 'ZN_ZN',\n",
       "       'INDUS_CRIM', 'INDUS_ZN', 'INDUS_INDUS', 'CHAS_CRIM', 'CHAS_ZN',\n",
       "       'CHAS_INDUS', 'CHAS_CHAS', 'NOX_CRIM', 'NOX_ZN', 'NOX_INDUS',\n",
       "       'NOX_CHAS', 'NOX_NOX', 'RM_CRIM', 'RM_ZN', 'RM_INDUS', 'RM_CHAS',\n",
       "       'RM_NOX', 'RM_RM', 'AGE_CRIM', 'AGE_ZN', 'AGE_INDUS', 'AGE_CHAS',\n",
       "       'AGE_NOX', 'AGE_RM', 'AGE_AGE', 'DIS_CRIM', 'DIS_ZN', 'DIS_INDUS',\n",
       "       'DIS_CHAS', 'DIS_NOX', 'DIS_RM', 'DIS_AGE', 'DIS_DIS', 'RAD_CRIM',\n",
       "       'RAD_ZN', 'RAD_INDUS', 'RAD_CHAS', 'RAD_NOX', 'RAD_RM', 'RAD_AGE',\n",
       "       'RAD_DIS', 'RAD_RAD', 'TAX_CRIM', 'TAX_ZN', 'TAX_INDUS',\n",
       "       'TAX_CHAS', 'TAX_NOX', 'TAX_RM', 'TAX_AGE', 'TAX_DIS', 'TAX_RAD',\n",
       "       'TAX_TAX', 'PTRATIO_CRIM', 'PTRATIO_ZN', 'PTRATIO_INDUS',\n",
       "       'PTRATIO_CHAS', 'PTRATIO_NOX', 'PTRATIO_RM', 'PTRATIO_AGE',\n",
       "       'PTRATIO_DIS', 'PTRATIO_RAD', 'PTRATIO_TAX', 'PTRATIO_PTRATIO',\n",
       "       'B_CRIM', 'B_ZN', 'B_INDUS', 'B_CHAS', 'B_NOX', 'B_RM', 'B_AGE',\n",
       "       'B_DIS', 'B_RAD', 'B_TAX', 'B_PTRATIO', 'B_B', 'LSTAT_CRIM',\n",
       "       'LSTAT_ZN', 'LSTAT_INDUS', 'LSTAT_CHAS', 'LSTAT_NOX', 'LSTAT_RM',\n",
       "       'LSTAT_AGE', 'LSTAT_DIS', 'LSTAT_RAD', 'LSTAT_TAX',\n",
       "       'LSTAT_PTRATIO', 'LSTAT_B', 'LSTAT_LSTAT', 'MEDV'], dtype=object)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bdf_tr.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((51, 105), (51,))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bdf_test.shape, bdf_test.MEDV.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Overfitting (sort of)\n",
    "Now, using your version of multivariate regression from 2.3, let's overfit the training data. Using your training set, regress housing price on as many of those K+(K*(K+1))/2 features as you can. If you get too greedy, it's possible this will take a long time to compute, so start with 5-10 features, and if you have the time, add more features.\n",
    "\n",
    "Keep track of the training RMSE each time you add a new feature. Plot a line graph with number of features on the x-axis and training RMSE on the y-axis. \n",
    "\n",
    "*NOTE-*\n",
    "* *You can set the MaxIterations as 1e5*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_rmse(predictions, yvalues):\n",
    "    pre = np.asarray(predictions)\n",
    "    y = np.asarray(yvalues)\n",
    "    rmse = np.sqrt(np.sum((pre-y) ** 2) / float(len(y)))\n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost < 0.00001 stopped after iteration times: 1228\n",
      "Time taken: 0.03 seconds\n",
      "cost < 0.00001 stopped after iteration times: 1952\n",
      "Time taken: 0.05 seconds\n",
      "cost < 0.00001 stopped after iteration times: 2035\n",
      "Time taken: 0.05 seconds\n",
      "cost < 0.00001 stopped after iteration times: 2424\n",
      "Time taken: 0.07 seconds\n",
      "cost < 0.00001 stopped after iteration times: 2280\n",
      "Time taken: 0.07 seconds\n",
      "cost < 0.00001 stopped after iteration times: 2539\n",
      "Time taken: 0.07 seconds\n",
      "cost < 0.00001 stopped after iteration times: 4380\n",
      "Time taken: 0.13 seconds\n",
      "cost < 0.00001 stopped after iteration times: 4399\n",
      "Time taken: 0.14 seconds\n",
      "cost < 0.00001 stopped after iteration times: 10914\n",
      "Time taken: 0.36 seconds\n",
      "cost < 0.00001 stopped after iteration times: 10962\n",
      "Time taken: 0.38 seconds\n",
      "cost < 0.00001 stopped after iteration times: 11081\n",
      "Time taken: 0.37 seconds\n",
      "cost < 0.00001 stopped after iteration times: 11297\n",
      "Time taken: 0.40 seconds\n",
      "cost < 0.00001 stopped after iteration times: 14459\n",
      "Time taken: 0.52 seconds\n",
      "cost < 0.00001 stopped after iteration times: 14555\n",
      "Time taken: 0.54 seconds\n",
      "cost < 0.00001 stopped after iteration times: 20742\n",
      "Time taken: 0.78 seconds\n",
      "cost < 0.00001 stopped after iteration times: 20779\n",
      "Time taken: 0.80 seconds\n",
      "cost < 0.00001 stopped after iteration times: 38967\n",
      "Time taken: 1.54 seconds\n",
      "Time taken: 40.78 seconds\n"
     ]
    }
   ],
   "source": [
    "lst=[]\n",
    "for j in range(2, 20):\n",
    "    features=[each for each in bdf_tr.columns[:j]]\n",
    "    x_tr=bdf_tr[features]\n",
    "    x_test=bdf_test[features]     \n",
    "    alpha, beta_array = multivariate_ols(x_tr, bdf_tr.MEDV, 0.01, int(1e6))\n",
    "    predict = np.zeros(shape=(len(bdf_tr.MEDV)))\n",
    "    predict= np.dot(x_tr,beta_array.squeeze(axis=1))\n",
    "    predict += alpha\n",
    "    rmse_tr = compute_rmse(np.array(predict), bdf_tr.MEDV)\n",
    "    \n",
    "    \n",
    "    predict_test = np.zeros(shape=(len(bdf_test.MEDV)))\n",
    "    predict_test= np.dot(x_test,beta_array.squeeze(axis=1))\n",
    "    predict_test += alpha\n",
    "    rmse_test = compute_rmse(np.array(predict_test), bdf_test.MEDV)\n",
    "    \n",
    "    \n",
    "    \n",
    "    dc={}\n",
    "    dc['NumberFeatures']=j\n",
    "    dc['RMSE_Training']=rmse_tr\n",
    "    dc['RMSE_Testing']=rmse_test\n",
    "    lst.append(dc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NumberFeatures</th>\n",
       "      <th>RMSE_Training</th>\n",
       "      <th>RMSE_Testing</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>8.265741</td>\n",
       "      <td>5.652394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>7.912296</td>\n",
       "      <td>5.909360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>7.689532</td>\n",
       "      <td>5.988461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>7.688834</td>\n",
       "      <td>5.984038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>6.142582</td>\n",
       "      <td>4.403738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>7</td>\n",
       "      <td>6.127639</td>\n",
       "      <td>4.347528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>8</td>\n",
       "      <td>5.862309</td>\n",
       "      <td>4.508963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>9</td>\n",
       "      <td>5.844632</td>\n",
       "      <td>4.494985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>10</td>\n",
       "      <td>5.780320</td>\n",
       "      <td>4.415250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>11</td>\n",
       "      <td>5.626921</td>\n",
       "      <td>3.942250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>12</td>\n",
       "      <td>5.465781</td>\n",
       "      <td>4.175380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>13</td>\n",
       "      <td>4.852266</td>\n",
       "      <td>4.495923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>14</td>\n",
       "      <td>4.848581</td>\n",
       "      <td>4.573822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>15</td>\n",
       "      <td>4.848091</td>\n",
       "      <td>4.574292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>16</td>\n",
       "      <td>4.835202</td>\n",
       "      <td>4.468585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>17</td>\n",
       "      <td>4.835206</td>\n",
       "      <td>4.468544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>18</td>\n",
       "      <td>4.814131</td>\n",
       "      <td>4.492202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>19</td>\n",
       "      <td>4.761322</td>\n",
       "      <td>4.619335</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    NumberFeatures  RMSE_Training  RMSE_Testing\n",
       "0                2       8.265741      5.652394\n",
       "1                3       7.912296      5.909360\n",
       "2                4       7.689532      5.988461\n",
       "3                5       7.688834      5.984038\n",
       "4                6       6.142582      4.403738\n",
       "5                7       6.127639      4.347528\n",
       "6                8       5.862309      4.508963\n",
       "7                9       5.844632      4.494985\n",
       "8               10       5.780320      4.415250\n",
       "9               11       5.626921      3.942250\n",
       "10              12       5.465781      4.175380\n",
       "11              13       4.852266      4.495923\n",
       "12              14       4.848581      4.573822\n",
       "13              15       4.848091      4.574292\n",
       "14              16       4.835202      4.468585\n",
       "15              17       4.835206      4.468544\n",
       "16              18       4.814131      4.492202\n",
       "17              19       4.761322      4.619335"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x14a200dbe10>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_42=pd.DataFrame(lst).set_index('NumberFeatures')\n",
    "# df_42['RMSE_Training'].plot(figsize=(8,6))\n",
    "# plt.show()\n",
    "# df_42['RMSE_Testing'].plot(figsize=(8,6))\n",
    "df_42['RMSE_Training'].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we increase the number of features, our RMSE drops in the training set. This is expected behaviour and is a sign of potential overfitting!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Ridge regularization\n",
    "Incorporate L2 (Ridge) regularization into your `multivariate_ols` regression. Write a new version of your gradient descent algorithm that includes a regularization term \"lambda\" to penalize excessive complexity. \n",
    "\n",
    "Use your regularized regression to re-fit the model using all features from the training data. Try this for several different values of lambda, and report your RMSE for each lambda separately for your training set. How do these numbers compare to each other, to the RMSE from 3.2?\n",
    "\n",
    "Go brag to your friends about how you just implemented ridge-regularized multivariate regression using gradient descent optimization, from scratch. If you still have friends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# incorporate alpha in the beta\n",
    "# adding a new column with same value one into the X\n",
    "\n",
    "def multivariate_regularized_ols(xvalue_matrix, y, R=0.01, MaxIterations=1000, lbd=1):\n",
    "    # initialize the parameters\n",
    "    yvalues = np.array(y)\n",
    "    start_time = time.time()\n",
    "    n = xvalue_matrix.shape[0]                                      #number of (training) data points\n",
    "    features = xvalue_matrix.shape[1]                               #number of features\n",
    "    iterr = 0\n",
    "    epsilon = 1e-6       \n",
    "    beta_array = np.random.random(features) \n",
    "    cost= np.inf\n",
    "    #beta_array = np.zeros((features))\n",
    "    alpha = 0.0\n",
    "    convergence=False\n",
    "    for i in range(MaxIterations):\n",
    "        iterr += 1\n",
    "\n",
    "        cost_old = cost\n",
    "        alpha_old = alpha\n",
    "        beta_array_old = beta_array\n",
    "        part_1 = alpha_old + np.dot(xvalue_matrix, beta_array_old) - yvalues\n",
    "        alpha -= R / n * (part_1.sum())\n",
    "        beta_array -= R / n * (np.dot(xvalue_matrix.T, part_1)+np.dot(lbd,beta_array_old))\n",
    "        part_2 = alpha + np.dot(xvalue_matrix, beta_array) - yvalues\n",
    "        cost = 1.0 / (2 * n) * ((part_2 ** 2).sum() + (lbd*(np.sum(beta_array**2))))\n",
    "        if cost_old - cost < 0:\n",
    "            break\n",
    "        \n",
    "#         g_b=np.dot(xvalue_matrix.T,np.dot(xvalue_matrix,beta_array)-y )+np.dot(lbd,beta_array)\n",
    "#         beta_array = beta_array - R/n *g_b \n",
    "\n",
    "        if abs(alpha-alpha_old) < epsilon and max(abs(beta_array-beta_array_old)) < epsilon:\n",
    "            convergence=True\n",
    "            ttaken=str(round(time.time() - start_time,2)) \n",
    "            break\n",
    "\n",
    "    ttaken=str(round(time.time() - start_time,2)) \n",
    "    return alpha,beta_array,ttaken,iterr,convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x,alpha,betas):\n",
    "    return alpha + np.dot(x,betas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lambda=0.001, R=0.01, timetaken=0.72, iterations=1229, convergence=True\n",
      "RMSE of training set: 3.75\n",
      "lambda=0.1, R=0.01, timetaken=0.72, iterations=1229, convergence=True\n",
      "RMSE of training set: 3.74\n",
      "lambda=1, R=0.01, timetaken=0.71, iterations=1229, convergence=True\n",
      "RMSE of training set: 3.75\n",
      "lambda=10, R=0.01, timetaken=0.72, iterations=1229, convergence=True\n",
      "RMSE of training set: 3.82\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "x_train = bdf_tr.iloc[:,:-1]\n",
    "x_train_tar = bdf_tr.MEDV\n",
    "\n",
    "\n",
    "x_test = bdf_test.iloc[:,:-1]\n",
    "x_test_tar = bdf_test.MEDV\n",
    "\n",
    "R = 0.01\n",
    "lbd = 0.001\n",
    "alpha,beta, timetaken, iterations, convergence = multivariate_regularized_ols(x_train, x_train_tar, R, 1000000,lbd)\n",
    "print(\"lambda={}, R={}, timetaken={}, iterations={}, convergence={}\".format(lbd,R,\n",
    "      timetaken,iterations,convergence))\n",
    "yhat_train = f(x_train,alpha,beta)\n",
    "yhat_test = f(x_test,alpha,beta)\n",
    "rmse_train = compute_rmse(yhat_train,x_train_tar)\n",
    "rmse_test = compute_rmse(yhat_test,x_test_tar)\n",
    "print(\"RMSE of training set: {:.2f}\".format(rmse_train))\n",
    "# print('Coeff:',beta)\n",
    "\n",
    "lbd = 0.1\n",
    "alpha,beta, timetaken, iterations, convergence = multivariate_regularized_ols(x_train, x_train_tar, R, 1000000,lbd)\n",
    "print(\"lambda={}, R={}, timetaken={}, iterations={}, convergence={}\".format(lbd, R,\n",
    "      timetaken,iterations,convergence))\n",
    "yhat_train = f(x_train,alpha,beta)\n",
    "yhat_test = f(x_test,alpha,beta)\n",
    "rmse_train = compute_rmse(yhat_train,x_train_tar)\n",
    "rmse_test = compute_rmse(yhat_test,x_test_tar)\n",
    "print(\"RMSE of training set: {:.2f}\".format(rmse_train))\n",
    "# print('Coeff:',beta)\n",
    "\n",
    "lbd = 1\n",
    "alpha,beta, timetaken, iterations, convergence = multivariate_regularized_ols(x_train, x_train_tar, R, 1000000,lbd)\n",
    "print(\"lambda={}, R={}, timetaken={}, iterations={}, convergence={}\".format(lbd,R,\n",
    "      timetaken,iterations,convergence))\n",
    "yhat_train = f(x_train,alpha,beta)\n",
    "yhat_test = f(x_test,alpha,beta)\n",
    "rmse_train = compute_rmse(yhat_train,x_train_tar)\n",
    "rmse_test = compute_rmse(yhat_test,x_test_tar)\n",
    "print(\"RMSE of training set: {:.2f}\".format(rmse_train))\n",
    "# print('Coeff:',beta)\n",
    "\n",
    "lbd = 10\n",
    "alpha,beta, timetaken, iterations, convergence = multivariate_regularized_ols(x_train, x_train_tar, R, 1000000,lbd)\n",
    "print(\"lambda={}, R={}, timetaken={}, iterations={}, convergence={}\".format(lbd,R,\n",
    "      timetaken,iterations,convergence))\n",
    "yhat_train = f(x_train,alpha,beta)\n",
    "yhat_test = f(x_test,alpha,beta)\n",
    "rmse_train = compute_rmse(yhat_train,x_train_tar)\n",
    "rmse_test = compute_rmse(yhat_test,x_test_tar)\n",
    "print(\"RMSE of training set: {:.2f}\".format(rmse_train))\n",
    "# print('Coeff:',beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Cross-validate lambda\n",
    "\n",
    "Up to this point, you have been fitting models and performing regularization (and basic tuning of the lambda hyperparameter) without cross-validation. This could easily lead to overfitting. Fix this by using 5-fold cross-validation on your training set to repeat 3.3 in a more systematic way.\n",
    "\n",
    "Specifically, use 5-fold cross-validation to select the optimal value of lambda, using the lambda values `[0.0001,0.001,0.01,0.1,1,10,100,1000,10000,100000]`. For each value of lambda, you should run 5-fold cross-validation, and report the average cross-validated RMSE across the 5 validation folds.  Create a figure that plots the average cross-validated RMSE (y-axis) as a function of lambda (x-axis).\n",
    "\n",
    "Finally, select the value of lambda that minimizes cross-validated RMSE, and use it to evaluate test performance (remember, this is your holdout set from 1.3!). How does the test RMSE compare to the cross-validated RMSE? How does it compare to RMSE from nearest neighbors in PS3?\n",
    "\n",
    "\n",
    "*Note-*:\n",
    "* Don't forget to use standardized data!\n",
    "* Your plot might be prettier if you show the x-axis in log scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000, 10000, 100000]\n"
     ]
    }
   ],
   "source": [
    "#lambdas = [0] + list(np.linspace(0.1, 1, num=10))+list(np.linspace(2, 10, num=9))+list(np.linspace(15, 100, num=18))\n",
    "lambdas = [0.0001,0.001,0.01,0.1,1,10,100,1000,10000,100000]\n",
    "print(lambdas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validated RMSE: 3.99\n",
      "\n",
      "Cross Validated RMSE: 4.02\n",
      "\n",
      "Cross Validated RMSE: 4.03\n",
      "\n",
      "Cross Validated RMSE: 3.99\n",
      "\n",
      "Cross Validated RMSE: 4.03\n",
      "\n",
      "Cross Validated RMSE: 4.09\n",
      "\n",
      "Cross Validated RMSE: 4.62\n",
      "\n",
      "Cross Validated RMSE: 5.32\n",
      "\n",
      "Cross Validated RMSE: 7.23\n",
      "\n",
      "Cross Validated RMSE: 88.18\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "x_train = bdf_tr.iloc[:,:-1]\n",
    "x_train_tar = bdf_tr.MEDV\n",
    "\n",
    "\n",
    "x_test = bdf_test.iloc[:,:-1]\n",
    "x_test_tar = bdf_test.MEDV\n",
    "\n",
    "dict_lbd_rmse_train = {}\n",
    "dict_lbd_rmse_test = {}\n",
    "for lbd in lambdas:\n",
    "    counter = 0.0\n",
    "    rmse_train_sum = 0\n",
    "    rmse_test_sum = 0\n",
    "    kf = KFold(n_splits=5)\n",
    "    for train_index, val_index in kf.split(x_train):\n",
    "        df_trainn = x_train.iloc[train_index]\n",
    "        df_val = x_train.iloc[val_index]\n",
    "        df_trainn_tar = x_train_tar.iloc[train_index]\n",
    "        df_val_tar = x_train_tar.iloc[val_index]\n",
    "        \n",
    "        R = 0.01\n",
    "        alpha,beta, timetaken, iterations, convergence = multivariate_regularized_ols(df_trainn, df_trainn_tar, R, 100000,lbd)\n",
    "\n",
    "        \n",
    "#         yhat = f(df_trainn,beta)\n",
    "        \n",
    "#         rmse_train = compute_rmse(yhat,df_trainn_tar)\n",
    "#         rmse_train_sum += rmse_train\n",
    "\n",
    "        yhat1 = f(df_val,alpha,beta)\n",
    "        rmse_test = compute_rmse(yhat1,df_val_tar)\n",
    "        rmse_test_sum += rmse_test\n",
    "        counter += 1.0\n",
    "\n",
    "#     dict_lbd_rmse_train[lbd] = rmse_train_sum / counter\n",
    "    dict_lbd_rmse_test[lbd] = rmse_test_sum / counter\n",
    "    print(\"Cross Validated RMSE: {:.2f}\\n\".format(rmse_test_sum / counter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x14a2015b198>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib as plt\n",
    "\n",
    "rmse_test_df=pd.DataFrame.from_dict(dict_lbd_rmse_test,orient='index').sort_index().reset_index()\n",
    "rmse_test_df.columns=['lambda','rmse_validation']\n",
    "rmse_test_df.set_index('lambda').plot(logx=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From 10e-3 to 10 we do not see much change in the RMSE (except for minor deviations), but beyond this the RMSE increases rapidly. The best lambda in this case is 0.001."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.771801993103173"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha,beta, timetaken, iterations, convergence = multivariate_regularized_ols(x_train, x_train_tar, 0.001, 100000,0.01)\n",
    "yhat1 = f(x_test,alpha,beta)\n",
    "rmse_test = compute_rmse(yhat1,x_test_tar)\n",
    "rmse_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Extra Credit 3: AdaGrad\n",
    "\n",
    "AdaGrad is a method to implement gradient descent with different learning rates for each feature. Adaptive algorithms like this one are being extensively used especially in deep learning applications. Implement AdaGrad on 2.3 but now use CRIM, RM and DIS as independent variables. Standardize these variables before before inputting them to the gradient descent algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function\n",
    "--------\n",
    "multivariate_ols\n",
    "    Gradient Decent to minimize OLS. \n",
    "    \n",
    "Parameters\n",
    "----------\n",
    "xvalue_matrix, yvalues : narray\n",
    "    xvalue_matrix: independent variable\n",
    "    yvalues: dependent variable\n",
    "    \n",
    "R: float\n",
    "    Learning rate\n",
    "    \n",
    "MaxIterations: Int\n",
    "    maximum number of iterations\n",
    "    \n",
    "\n",
    "Returns\n",
    "-------\n",
    "alpha: float\n",
    "    intercept\n",
    "    \n",
    "beta_array: array[float]\n",
    "    co-efficient\n",
    "\"\"\"\n",
    "\n",
    "def AdaGrad_ols(xvalue_matrix, yvalues, R=0.00001, MaxIterations=10000):\n",
    "    start_time = time.time()\n",
    "    xvalue_matrix = np.array(xvalue_matrix)\n",
    "    yvalues = np.array(yvalues)[:, np.newaxis]\n",
    "    n, variable_n = xvalue_matrix.shape # sample number, varaibels number\n",
    "    # initialize parameters\n",
    "    params = np.random.randn(variable_n + 1, 1)\n",
    "    cost = np.inf\n",
    "    epsilon = 0.00001\n",
    "    G = np.zeros((variable_n + 1, variable_n + 1))\n",
    "    for i in range(MaxIterations):\n",
    "        g = np.zeros((variable_n + 1, 1))\n",
    "        cost_old = cost\n",
    "        part_1 = params[0] + np.dot(xvalue_matrix, params[1:variable_n + 1]) - yvalues\n",
    "        # populate gradient vector\n",
    "        g[0] = part_1.sum()\n",
    "        g[1:variable_n + 1] = np.dot(xvalue_matrix.T, part_1)\n",
    "        # store outer product of past values of gradients \n",
    "        G += np.outer(g, g)\n",
    "        # update parameters (gd steps)\n",
    "        for j in range(variable_n + 1):\n",
    "            params[j] -= R / n * g[j]/np.sqrt(np.diag(G)[j])\n",
    "        part_2 = params[0] + np.dot(xvalue_matrix, params[1:variable_n + 1]) - yvalues\n",
    "        #print(params[2])\n",
    "        cost = 1.0 / (2 * n) * ((part_2 ** 2).sum())\n",
    "    print(\"Time taken: {:.2f} seconds\".format(time.time() - start_time))\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 1\n",
      "Time taken: 1.71 seconds\n",
      "[[22.56145211]\n",
      " [ 5.78244049]\n",
      " [-2.23526742]\n",
      " [ 0.22097372]]\n"
     ]
    }
   ],
   "source": [
    "df_cp = df_train.copy()\n",
    "df_norm = standardize(df_cp)\n",
    "#IPython.embed()\n",
    "df_norm['MEDV'] = df_train['MEDV']\n",
    "print('Test 1')\n",
    "print(AdaGrad_ols(df_norm[['RM', 'CRIM', 'DIS']], df_norm['MEDV'], 100.5, 10000))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intercept: 22.69\n",
      "Coefficient of RM: 5.78\n",
      "Coefficient of CRIM: -2.24\n",
      "Coefficient of DIS: 0.22\n"
     ]
    }
   ],
   "source": [
    "regr_4 = LinearRegression()\n",
    "regr_4.fit(df_norm[['RM', 'CRIM','DIS']], df_norm['MEDV'])\n",
    "print('Intercept: {:.2f}'.format(regr_4.intercept_))\n",
    "print('Coefficient of RM: {:.2f}'.format(regr_4.coef_[0]))\n",
    "print('Coefficient of CRIM: {:.2f}'.format(regr_4.coef_[1]))\n",
    "print('Coefficient of DIS: {:.2f}'.format(regr_4.coef_[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
