{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Set 5\n",
    "\n",
    "As usual, you are highly encouraged to *start this problem set early!*  Our best guess (no guarantees!) is that Part 1 will be conceptually easy but perhaps time consuming.  Parts 2 and 3 are medium difficult, but there are plenty of opportunities to get confused with data structures, so if you're going to get stuck, get stuck (and unstuck) early!\n",
    "\n",
    "Note that the extra credit problems have the potential to be quite challenging and time consuming. Make sure to do everything else in the problem set before you tackle the extra credit.\n",
    "\n",
    "To get started, download the files \"reviews.csv\" and \"movies.dat\" from Canvas.\n",
    "\n",
    "In addition to numpy, pandas, sklearn and matplotlib, we suggest you familiarize yourself with the following aspects of these libraries, which are not required but which might make your life much easier:-\n",
    "\n",
    "* `DataFrame` and other basic [data structures](http://pandas.pydata.org/pandas-docs/stable/dsintro.html) in pandas\n",
    "* [`groupby`](http://pandas.pydata.org/pandas-docs/stable/groupby.html), `unique`,  and other simple aggregation functions\n",
    "* [`CountVectorizer`](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) for working with text in sklearn.\n",
    "* [`MultinomialNB`](http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html) for Naive Bayes\n",
    "* Simple tools for [Cross-Validation](http://scikit-learn.org/stable/modules/cross_validation.html) in sklearn\n",
    "\n",
    "Make sure they load properly with the following code block. You may also import additional functions/classes if you need them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#You can go over the scikit-learn documentation to learn more about what some of these functions and classes\n",
    "from  sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import IPython"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to the assignment\n",
    "\n",
    "In this assignment, you'll be analyzing movie reviews in an attempt to determine whether movies are good or bad.  We have downloaded a large number of movie reviews from the [Rotten Tomatoes](http://www.rottentomatoes.com) website, which are available in the file \"reviews.csv\" on Canvas.  We have also put a file \"movies.dat\" on Canvas that contains metadata for ~65,000 different movies.\n",
    "\n",
    "If you've never seen Rotten Tomatoes before, go spend a few minutes familiarizing yourself with the website.  It's a meta-review website that aggregates movie reviews from a large number of critics.  Here's what the page for 2020's [Oscar winner](https://www.rottentomatoes.com/m/parasite_2019) looks like.  Note in particular the section on \"Critic Reviews\" at the bottom -- that's the content that has been scraped and put in the reviews.csv file.  Also note that each movie is given a quantitative score on the Tomatometer which is used to classify the movie as either Fresh (good) or Rotten (bad).\n",
    "\n",
    "Your mission is to develop a classifier to determine whether a movie is Fresh or Rotten based on the contents of the reviews given to the movie.  As usual, we'll start out with some exploratory data analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Descriptive analysis\n",
    "\n",
    "Before beginning the \"real\" work, dig into the data a bit do see what you're dealing with.  Let's start by loading the datasets and dropping rows that have missing data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = pd.read_csv('reviews.csv')\n",
    "movies = pd.read_csv(\"movies.dat\", delimiter='\\t')\n",
    "movies = movies.dropna()\n",
    "reviews = reviews[~reviews.quote.isnull()]\n",
    "reviews = reviews[reviews.fresh != 'none']\n",
    "reviews = reviews[reviews.quote.str.len() > 0]\n",
    "reviews = reviews[~reviews.critic.isna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1: Basic features of the dataset\n",
    "\n",
    "Answer the following questions by having your Python code directly print the answers:\n",
    "\n",
    "* How many unique reviews (quotes) are in the `reviews` dataset?\n",
    "* How many unique reviewers (critics) are in the `reviews` dataset?\n",
    "* How many unique movies are in the `reviews` dataset?\n",
    "* How many different publications have reviews in the `reviews` dataset?\n",
    "* How many unique movies are in the `movies` dataset?\n",
    "* What is the year range in the `movies` dataset? \n",
    "* What is the lowest `rtAudienceScore` for the year 1995 in the `movies` dataset?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12149"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# unique reviews (quotes)\n",
    "len(reviews['quote'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "540"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#unique reviewers (critics)\n",
    "len(reviews['critic'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1715"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#unique movies\n",
    "len(reviews['title'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#different publications\n",
    "len(reviews['publication'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8959"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#unique movies are in the movies\n",
    "len(movies['title'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "108"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#year range in the movies\n",
    "movies['year'].max() - movies['year'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rtAudienceScore       0\n",
       "year               1995\n",
       "dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#the lowest rtAudienceScore for the year 1995 in the movies\n",
    "df_95 = movies[['rtAudienceScore', 'year']]\n",
    "df_95.loc[df_95['year'] == 1995].sort_values(by='rtAudienceScore').min()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Number of reviews per critic\n",
    "\n",
    "Using the `reviews` dataset, create a histogram showing the distribution of the number of reviews per critic.  Have the bin width (x-axis) be 5 units wide, and the range of the x-axis to be from 0 to 100.  Scale the y-axis to show the logarithm of the (count of) the number of critics.  Also add a vertical line at the average number of reviews per critic. \n",
    "\n",
    "Label your axes and make your histogram look professional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-10-e5e833ae83fa>:8: MatplotlibDeprecationWarning: The 'nonposy' parameter of __init__() has been renamed 'nonpositive' since Matplotlib 3.3; support for the old name will be dropped two minor releases later.\n",
      "  plt.yscale('log', nonposy='clip')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Histogram Plot of the number of reviews per critic')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# the distribution of the number of reviews per critic\n",
    "df_cr = reviews[['critic']].value_counts()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#range of x_axis, logarithm y_axis\n",
    "plt.xlim(xmin=0, xmax = 100)\n",
    "plt.yscale('log', nonposy='clip')\n",
    "\n",
    "#bin width be 5 units wide\n",
    "binwidth = 5\n",
    "plt.hist(df_cr, bins = np.arange(min(df_cr), max(df_cr) + binwidth, binwidth))\n",
    "\n",
    "plt.axvline(df_cr.mean(), color='r', linewidth=1)\n",
    "\n",
    "plt.xlabel('critic')\n",
    "plt.ylabel('number of reviews (log)')\n",
    "plt.title('Histogram Plot of the number of reviews per critic')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Zoom in on a few reviewers\n",
    "Find the 25 critics with the most reviews, and list their names in a table along with -\n",
    "\n",
    "* (a) The name(s) of *all* the publications they work for. If a critic happens to work for more than one publication, you should have a comma-separated string that contains all publications for that critic.\n",
    "* (b) The date of their first review.\n",
    "* (c) The date of their last review.\n",
    "* (d) The total number of reviews (count).\n",
    "\n",
    "Sort the table in descending order of the number of reviews.\n",
    "\n",
    "*Hint: The `apply`, `merge` and `groupby` functions can be useful here*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataframe with top 25 critics count\n",
    "df_top25 = df_cr.iloc[0:25].to_frame()\n",
    "df_top25 = df_top25.reset_index()\n",
    "df_top25.columns = ['critic', 'count']\n",
    "df_top25['publication'] = np.nan\n",
    "df_top25['first review'] = np.nan\n",
    "df_top25['last review'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_25_list = df_top25['critic'].to_list()\n",
    "first_review = []\n",
    "last_review = []\n",
    "publication = []\n",
    "\n",
    "for name in top_25_list:\n",
    "    re = reviews[reviews['critic'] == name]\n",
    "    pub = re['publication'].unique().tolist()\n",
    "    \n",
    "    #append the min date of review\n",
    "    min_date = min(re['review_date'])\n",
    "    first_review.append(min_date)\n",
    "    \n",
    "    #append the max data of review\n",
    "    max_date = max(re['review_date'])\n",
    "    last_review.append(max_date)\n",
    "    \n",
    "    #concatenate publications\n",
    "    str1 = ', '.join(pub)\n",
    "\n",
    "    publication.append(str1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>critic</th>\n",
       "      <th>count</th>\n",
       "      <th>publication</th>\n",
       "      <th>first review</th>\n",
       "      <th>last review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Roger Ebert</td>\n",
       "      <td>1150</td>\n",
       "      <td>Chicago Sun-Times, At the Movies</td>\n",
       "      <td>2000-01-01 00:00:00</td>\n",
       "      <td>2013-06-11 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>James Berardinelli</td>\n",
       "      <td>833</td>\n",
       "      <td>ReelViews</td>\n",
       "      <td>1800-01-01 00:00:00</td>\n",
       "      <td>2012-02-07 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Janet Maslin</td>\n",
       "      <td>526</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>2000-01-01 00:00:00</td>\n",
       "      <td>2010-09-07 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Variety Staff</td>\n",
       "      <td>449</td>\n",
       "      <td>Variety</td>\n",
       "      <td>2001-02-13 00:00:00</td>\n",
       "      <td>2012-08-15 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Jonathan Rosenbaum</td>\n",
       "      <td>420</td>\n",
       "      <td>Chicago Reader</td>\n",
       "      <td>1988-07-11 00:00:00</td>\n",
       "      <td>2013-10-22 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Desson Thomson</td>\n",
       "      <td>373</td>\n",
       "      <td>Washington Post</td>\n",
       "      <td>2000-01-01 00:00:00</td>\n",
       "      <td>2012-10-08 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Geoff Andrew</td>\n",
       "      <td>361</td>\n",
       "      <td>Time Out</td>\n",
       "      <td>2005-08-11 00:00:00</td>\n",
       "      <td>2012-04-05 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Dave Kehr</td>\n",
       "      <td>360</td>\n",
       "      <td>Chicago Reader, New York Daily News, Chicago T...</td>\n",
       "      <td>2000-01-01 00:00:00</td>\n",
       "      <td>2013-11-20 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Kenneth Turan</td>\n",
       "      <td>282</td>\n",
       "      <td>Los Angeles Times</td>\n",
       "      <td>2000-01-01 00:00:00</td>\n",
       "      <td>2013-10-05 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Mick LaSalle</td>\n",
       "      <td>280</td>\n",
       "      <td>San Francisco Chronicle</td>\n",
       "      <td>2000-01-01 00:00:00</td>\n",
       "      <td>2009-11-25 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Owen Gleiberman</td>\n",
       "      <td>241</td>\n",
       "      <td>Entertainment Weekly</td>\n",
       "      <td>1800-01-01 00:00:00</td>\n",
       "      <td>2011-09-07 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Rita Kempley</td>\n",
       "      <td>236</td>\n",
       "      <td>Washington Post</td>\n",
       "      <td>2000-01-01 00:00:00</td>\n",
       "      <td>2013-11-02 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Joe Baltake</td>\n",
       "      <td>231</td>\n",
       "      <td>Sacramento Bee, Passionate Moviegoer, Philadel...</td>\n",
       "      <td>2000-01-01 00:00:00</td>\n",
       "      <td>2013-08-04 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Todd McCarthy</td>\n",
       "      <td>223</td>\n",
       "      <td>Variety</td>\n",
       "      <td>2000-01-01 00:00:00</td>\n",
       "      <td>2012-10-09 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Vincent Canby</td>\n",
       "      <td>207</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>2000-01-01 00:00:00</td>\n",
       "      <td>2013-07-29 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Hal Hinson</td>\n",
       "      <td>174</td>\n",
       "      <td>Washington Post</td>\n",
       "      <td>2000-01-01 00:00:00</td>\n",
       "      <td>2013-06-26 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Susan Stark</td>\n",
       "      <td>167</td>\n",
       "      <td>Detroit News</td>\n",
       "      <td>2000-01-01 00:00:00</td>\n",
       "      <td>2002-10-25 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Stephen Holden</td>\n",
       "      <td>166</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>2000-01-01 00:00:00</td>\n",
       "      <td>2009-02-06 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Derek Adams</td>\n",
       "      <td>154</td>\n",
       "      <td>Time Out</td>\n",
       "      <td>2006-01-26 00:00:00</td>\n",
       "      <td>2009-10-04 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Kevin Thomas</td>\n",
       "      <td>145</td>\n",
       "      <td>Los Angeles Times</td>\n",
       "      <td>2000-01-01 00:00:00</td>\n",
       "      <td>2013-11-02 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Peter Stack</td>\n",
       "      <td>138</td>\n",
       "      <td>San Francisco Chronicle</td>\n",
       "      <td>2000-01-01 00:00:00</td>\n",
       "      <td>2002-06-18 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Bosley Crowther</td>\n",
       "      <td>137</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>2000-01-01 00:00:00</td>\n",
       "      <td>2008-05-06 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Rick Groen</td>\n",
       "      <td>129</td>\n",
       "      <td>Globe and Mail</td>\n",
       "      <td>2002-03-19 00:00:00</td>\n",
       "      <td>2009-08-14 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Edward Guthmann</td>\n",
       "      <td>121</td>\n",
       "      <td>San Francisco Chronicle</td>\n",
       "      <td>2000-01-01 00:00:00</td>\n",
       "      <td>2003-04-04 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Lisa Schwarzbaum</td>\n",
       "      <td>117</td>\n",
       "      <td>Entertainment Weekly</td>\n",
       "      <td>1979-01-01 00:00:00</td>\n",
       "      <td>2012-08-15 00:00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                critic  count  \\\n",
       "0          Roger Ebert   1150   \n",
       "1   James Berardinelli    833   \n",
       "2         Janet Maslin    526   \n",
       "3        Variety Staff    449   \n",
       "4   Jonathan Rosenbaum    420   \n",
       "5       Desson Thomson    373   \n",
       "6         Geoff Andrew    361   \n",
       "7            Dave Kehr    360   \n",
       "8        Kenneth Turan    282   \n",
       "9         Mick LaSalle    280   \n",
       "10     Owen Gleiberman    241   \n",
       "11        Rita Kempley    236   \n",
       "12         Joe Baltake    231   \n",
       "13       Todd McCarthy    223   \n",
       "14       Vincent Canby    207   \n",
       "15          Hal Hinson    174   \n",
       "16         Susan Stark    167   \n",
       "17      Stephen Holden    166   \n",
       "18         Derek Adams    154   \n",
       "19        Kevin Thomas    145   \n",
       "20         Peter Stack    138   \n",
       "21     Bosley Crowther    137   \n",
       "22          Rick Groen    129   \n",
       "23     Edward Guthmann    121   \n",
       "24    Lisa Schwarzbaum    117   \n",
       "\n",
       "                                          publication         first review  \\\n",
       "0                    Chicago Sun-Times, At the Movies  2000-01-01 00:00:00   \n",
       "1                                           ReelViews  1800-01-01 00:00:00   \n",
       "2                                      New York Times  2000-01-01 00:00:00   \n",
       "3                                             Variety  2001-02-13 00:00:00   \n",
       "4                                      Chicago Reader  1988-07-11 00:00:00   \n",
       "5                                     Washington Post  2000-01-01 00:00:00   \n",
       "6                                            Time Out  2005-08-11 00:00:00   \n",
       "7   Chicago Reader, New York Daily News, Chicago T...  2000-01-01 00:00:00   \n",
       "8                                   Los Angeles Times  2000-01-01 00:00:00   \n",
       "9                             San Francisco Chronicle  2000-01-01 00:00:00   \n",
       "10                               Entertainment Weekly  1800-01-01 00:00:00   \n",
       "11                                    Washington Post  2000-01-01 00:00:00   \n",
       "12  Sacramento Bee, Passionate Moviegoer, Philadel...  2000-01-01 00:00:00   \n",
       "13                                            Variety  2000-01-01 00:00:00   \n",
       "14                                     New York Times  2000-01-01 00:00:00   \n",
       "15                                    Washington Post  2000-01-01 00:00:00   \n",
       "16                                       Detroit News  2000-01-01 00:00:00   \n",
       "17                                     New York Times  2000-01-01 00:00:00   \n",
       "18                                           Time Out  2006-01-26 00:00:00   \n",
       "19                                  Los Angeles Times  2000-01-01 00:00:00   \n",
       "20                            San Francisco Chronicle  2000-01-01 00:00:00   \n",
       "21                                     New York Times  2000-01-01 00:00:00   \n",
       "22                                     Globe and Mail  2002-03-19 00:00:00   \n",
       "23                            San Francisco Chronicle  2000-01-01 00:00:00   \n",
       "24                               Entertainment Weekly  1979-01-01 00:00:00   \n",
       "\n",
       "            last review  \n",
       "0   2013-06-11 00:00:00  \n",
       "1   2012-02-07 00:00:00  \n",
       "2   2010-09-07 00:00:00  \n",
       "3   2012-08-15 00:00:00  \n",
       "4   2013-10-22 00:00:00  \n",
       "5   2012-10-08 00:00:00  \n",
       "6   2012-04-05 00:00:00  \n",
       "7   2013-11-20 00:00:00  \n",
       "8   2013-10-05 00:00:00  \n",
       "9   2009-11-25 00:00:00  \n",
       "10  2011-09-07 00:00:00  \n",
       "11  2013-11-02 00:00:00  \n",
       "12  2013-08-04 00:00:00  \n",
       "13  2012-10-09 00:00:00  \n",
       "14  2013-07-29 00:00:00  \n",
       "15  2013-06-26 00:00:00  \n",
       "16  2002-10-25 00:00:00  \n",
       "17  2009-02-06 00:00:00  \n",
       "18  2009-10-04 00:00:00  \n",
       "19  2013-11-02 00:00:00  \n",
       "20  2002-06-18 00:00:00  \n",
       "21  2008-05-06 00:00:00  \n",
       "22  2009-08-14 00:00:00  \n",
       "23  2003-04-04 00:00:00  \n",
       "24  2012-08-15 00:00:00  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#convert lists to pd.series and append to df\n",
    "last_review = pd.Series(last_review)\n",
    "df_top25['last review'] = last_review\n",
    "\n",
    "first_review = pd.Series(first_review)\n",
    "df_top25['first review'] = first_review\n",
    "\n",
    "publication = pd.Series(publication)\n",
    "df_top25['publication'] = publication\n",
    "df_top25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Ratings over time\n",
    "\n",
    "Using the `movies` dataset, create a plot that shows how the average movie rating has evolved over time. Refer to `rtAllCriticsRating` for each movie's rating. \n",
    "\n",
    "Do this by creating a scatterplot where the x-axis is the year in which the movie was released and the y-axis is `rtAllCriticsRating`.  Drop movies with 0 or unknown values to simplify the analysis.  Then, create a scatterplot that has one <year,rating> point for each remaining movie, and pick an appropriate size/color/transparency for these points to ensure that the graph looks professional.  In other words, do not simply use the default settings, as this will produce a dense mess of dots that will be hard to interpret.  Finally, overlay on this scatterplot a line graph in red showing how the average movie rating (the average of `rtAllCriticsRating` across all movies in a year) has changed over time.  \n",
    "\n",
    "Do you notice a trend?  What do you think it means?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>year</th>\n",
       "      <th>rtAllCriticsRating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Toy story</td>\n",
       "      <td>1995</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Jumanji</td>\n",
       "      <td>1995</td>\n",
       "      <td>5.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Grumpy Old Men</td>\n",
       "      <td>1993</td>\n",
       "      <td>5.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Waiting to Exhale</td>\n",
       "      <td>1995</td>\n",
       "      <td>5.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Father of the Bride Part II</td>\n",
       "      <td>1995</td>\n",
       "      <td>5.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9414</th>\n",
       "      <td>Pete Seeger: The Power of Song</td>\n",
       "      <td>2007</td>\n",
       "      <td>8.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9418</th>\n",
       "      <td>Ben X</td>\n",
       "      <td>2007</td>\n",
       "      <td>6.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9419</th>\n",
       "      <td>Bedtime Stories</td>\n",
       "      <td>2008</td>\n",
       "      <td>4.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9420</th>\n",
       "      <td>Manhattan Melodrama</td>\n",
       "      <td>1934</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9421</th>\n",
       "      <td>Choke</td>\n",
       "      <td>2008</td>\n",
       "      <td>5.6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7616 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                               title  year  rtAllCriticsRating\n",
       "0                          Toy story  1995                 9.0\n",
       "1                            Jumanji  1995                 5.6\n",
       "2                     Grumpy Old Men  1993                 5.9\n",
       "3                  Waiting to Exhale  1995                 5.6\n",
       "4        Father of the Bride Part II  1995                 5.3\n",
       "...                              ...   ...                 ...\n",
       "9414  Pete Seeger: The Power of Song  2007                 8.2\n",
       "9418                           Ben X  2007                 6.3\n",
       "9419                 Bedtime Stories  2008                 4.4\n",
       "9420             Manhattan Melodrama  1934                 7.0\n",
       "9421                           Choke  2008                 5.6\n",
       "\n",
       "[7616 rows x 3 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create df for plotting\n",
    "plot1 = movies[['title', 'year','rtAllCriticsRating']]\n",
    "plot1 = plot1.astype({'rtAllCriticsRating': np.float})\n",
    "plot1_filter = plot1[plot1['rtAllCriticsRating'] != 0.0]\n",
    "plot1_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plot the graph\n",
    "rating_over_time  = plot1_filter.groupby('year')['rtAllCriticsRating'].mean()\n",
    "df = rating_over_time.to_frame()\n",
    "df.reset_index(inplace = True)\n",
    "df.columns = ['year', 'ratings']\n",
    "df.plot(kind= 'scatter', x= 'year', y= 'ratings')\n",
    "\n",
    "p = np.polyfit(df.year, df.ratings, 3)\n",
    "plt.plot(df.year,np.polyval(p, df.year),'r-')\n",
    "plt.title('movie ratings over time')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the trend that show in the graph, the average rating of movies is decreasing over the years. Probably because people are more critical or the production quality of films decreased over time indeed. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Naive Bayes\n",
    "\n",
    "Now it gets fun!  You are going to use a [Naive Bayes Classifier](https://en.wikipedia.org/wiki/Naive_Bayes_classifier) to build a prediction model for whether a review is fresh or rotten, depending on the text of the review. Review the assigned readings on Canvas, as well as the relevant lecture notes before embarking on this journey.\n",
    "\n",
    "### Using CountVectorizer\n",
    "\n",
    "One thing that may help you in the following problems is the [CountVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) class in Scikit-learn.  This will help you convert your raw text fields into \"bag of words\" vectors, i.e. a data structure that tells you how many times a particular word appears in a blurb.  Here's a simple example, make sure you understand what's going on!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text is:-\n",
      " machine learning rocks\n",
      "machine learning rules\n",
      "rocks rocks rules\n",
      "\n",
      "Transformed text vector is \n",
      " [[1 1 1 0]\n",
      " [1 1 0 1]\n",
      " [0 0 2 1]]\n",
      "\n",
      "Words for each feature:-\n",
      "['learning', 'machine', 'rocks', 'rules']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "text = ['machine learning rocks', 'machine learning rules', 'rocks rocks rules']\n",
    "print(\"Original text is:-\\n\", '\\n'.join(text))\n",
    "print()\n",
    "vectorizer = CountVectorizer(min_df=0)\n",
    "\n",
    "# call `fit` to build the vocabulary\n",
    "vectorizer.fit(text)\n",
    "\n",
    "# call `transform` to convert text to a bag of words\n",
    "x = vectorizer.transform(text)\n",
    "\n",
    "# CountVectorizer uses a sparse array to save memory, but it's easier in this assignment to \n",
    "# convert back to a \"normal\" numpy array\n",
    "x = x.toarray()\n",
    "\n",
    "print(\"Transformed text vector is \\n\", x)\n",
    "print()\n",
    "# `get_feature_names` tracks which word is associated with each column of the transformed x\n",
    "print(\"Words for each feature:-\")\n",
    "print(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Create your X input and Y output\n",
    "\n",
    "Using the `reviews` dataframe, compute a pair of numerical X, Y arrays where -\n",
    "    \n",
    " * X is an **(nreview, nwords)** array. Each row corresponds to a bag-of-words representation for a single review. This will be the *input* to your model.\n",
    " * Y is an **nreview**-element 1/0 array, encoding whether a review is Fresh (1) or Rotten (0). This is the desired *output* from your model.\n",
    " \n",
    "Make sure to remove items with no review text, if any. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function\n",
    "--------\n",
    "make_xy\n",
    "\n",
    "Build a bag-of-words training set for the review data\n",
    "\n",
    "Parameters\n",
    "-----------\n",
    "reviews : pandas DataFrame\n",
    "    The review data from above\n",
    "    \n",
    "vectorizer : CountVectorizer object (optional)\n",
    "    A CountVectorizer object to use. If None,\n",
    "    then create and fit a new CountVectorizer.\n",
    "    Otherwise, re-fit the provided CountVectorizer\n",
    "    using the reviews data\n",
    "    \n",
    "Returns\n",
    "-------\n",
    "X : numpy array (dims: nreview, nwords)\n",
    "    Bag-of-words representation for each review.\n",
    "Y : numpy array (dims: nreview)\n",
    "    1/0 array. 1 = fresh review, 0 = rotten review\n",
    "\n",
    "Examples\n",
    "--------\n",
    "X, Y = make_xy(reviews)\n",
    "\"\"\"\n",
    "def make_xy(reviews, vectorizer):\n",
    "    #subset the dataframe\n",
    "    df_bow = reviews[['quote','fresh']]\n",
    "    \n",
    "    # Data of fresh is converted into Binary Data\n",
    "    df_one = pd.get_dummies(df_bow[\"fresh\"])\n",
    "    # Binary Data is Concatenated into Dataframe\n",
    "    df_two = pd.concat((df_one, df_bow), axis=1)\n",
    "    # rename columns\n",
    "    df_two.columns = ['output', 'rotten', 'quote','fresh']\n",
    "    # columns droped\n",
    "    df_two = df_two.drop([\"fresh\"], axis=1)\n",
    "    y = df_two.drop([\"rotten\"], axis=1)\n",
    "    #output y\n",
    "    Y = y['output'].to_numpy()\n",
    "    \n",
    "    #checking vectorizer parameter\n",
    "    if vectorizer == None:\n",
    "        count_v = CountVectorizer(min_df = 0)\n",
    "    else:\n",
    "        count_v = vectorizer\n",
    "    \n",
    "    w = count_v.fit(df_bow['quote'])\n",
    "    w = count_v.transform(df_bow['quote'])\n",
    "    X = w.toarray()\n",
    "\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = make_xy(reviews, vectorizer = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12718, 20267)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12718,)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Test-Train split\n",
    "\n",
    "Next, randomly split the data into two groups: a training set (80%) and a validation set (20%).  You can do this manually, as you did in the prior problem set, or use [`train_test_split`](http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.train_test_split.html) to do this auto-magically.  If you use `train_test_split`, set the `random_state` to 42, and have the split be stratified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a training set (80%) and a validation set (20%)\n",
    "X_train, X_test, y_train, y_test= train_test_split(X, Y, test_size= .2, random_state = 42, stratify= Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Naive Bayes with `MultinomialNB`\n",
    "Use the training set to train a Naive Bayes classifier using [`MultinomialNB`](http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html).  Report the accuracy of this model on both the training and testing data.  What do you observe?  Interpret these results.\n",
    "\n",
    "*Hint: This shouldn't take more than 5-10 lines of code to accomplish*\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set Accuracy Score: \n",
      " 92.70689994102614\n",
      "Testing Set Accuracy Score: \n",
      " 76.69025157232704\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "clf = MultinomialNB()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "#predictions\n",
    "y_train_pred = clf.predict(X_train)\n",
    "y_test_pred = clf.predict(X_test)\n",
    "\n",
    "#accuracy score \n",
    "train_pred_score = accuracy_score(y_train, y_train_pred)\n",
    "test_pred_score = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "print('Training Set Accuracy Score: \\n', (100 * train_pred_score))\n",
    "print('Testing Set Accuracy Score: \\n', (100 * test_pred_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- the training set has a higher accuracy score of 92.7%, since we fit the data on the training data;\n",
    "- the testing set has a lower accuracy score or 76.7%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 (EXTRA CREDIT) Naive Bayes from Scratch!\n",
    "That was almost too easy, right?  Right.\n",
    "\n",
    "Your next mission, should you choose to accept it, is to write your own Naive Bayes classifier without relying on `MultinomialNB` or a similar pre-written package.  In addition to the lecture notes and assigned readings, I recommend that you review Michael Collin’s lecture notes on Naïve Bayes before starting (available on bcourses).  \n",
    "\n",
    "**Note:**\n",
    "You should do this extra credit assignment *after* you have finished the rest of the problem set.  It is very rewarding, but can also be quite time-consuming!\n",
    "\n",
    "*Hint: You will benefit most from this exercise if you attempt to write the algorithm directly from the lecture notes.  That said, if you really get stuck, Krishnamurthy Viswanathan has written a simple version of [NB in python](http://ebiquity.umbc.edu/blogger/2010/12/07/naive-bayes-classifier-in-50-lines/), which you can peek at if you really get stuck.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------\n",
    "\n",
    "## Part 3: Evaluation\n",
    "\n",
    "### 3.1 Estimate the likelihood of your data\n",
    "\n",
    "Given a fitted model, you can compute the log-likelihood of your data as a way to assess the performance of your model.  Using `fitted_model.predict_logproba`, the idea is to write a function that computes the log-likelihood of a dataset, so that we can inspect the log-likelihood of your training and testing data given your fitted model from part 2.\n",
    "\n",
    "To help you out a little bit, we'll do this part for you. But make sure you understand it, because you'll need to use this later!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1484.6226919350693"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Function\n",
    "--------\n",
    "log_likelihood\n",
    "\n",
    "Compute the log likelihood of a dataset according to a bayesian classifier. \n",
    "The Log Likelihood is defined by\n",
    "\n",
    "L = Sum_fresh(logP(fresh)) + Sum_rotten(logP(rotten))\n",
    "\n",
    "Where Sum_fresh indicates a sum over all fresh reviews, \n",
    "and Sum_rotten indicates a sum over rotten reviews\n",
    "    \n",
    "Parameters\n",
    "----------\n",
    "model : Bayesian classifier\n",
    "x : (nexample, nfeature) array\n",
    "    The input data\n",
    "y : (nexample) integer array\n",
    "    Whether each review is Fresh\n",
    "\"\"\"\n",
    "\n",
    "def log_likelihood(model, x, y):\n",
    "    prob = model.predict_log_proba(x)\n",
    "    rotten = y == 0\n",
    "    fresh = ~rotten\n",
    "    return prob[rotten, 0].sum() + prob[fresh, 1].sum()\n",
    "\n",
    "# output the likelihood of your test data (example - you may need to \n",
    "# change the names of the variables below to match your code in 2.2 and 2.3\n",
    "log_likelihood(clf, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Cross-Validation\n",
    "\n",
    "Why do we care about the log-likelihood of our data?  You guessed it: Cross-Validation.\n",
    "\n",
    "Our methods have a few hyperparameters. The two most important are:\n",
    "\n",
    " 1. The `min_df` keyword in `CountVectorizer`, which will ignore words which appear in fewer than `min_df` fraction of reviews. Words that appear only once or twice can lead to overfitting, since words which occur only a few times might correlate very well with Fresh/Rotten reviews by chance in the training dataset.\n",
    " \n",
    " 2. The [`alpha`](http://scikit-learn.org/stable/modules/naive_bayes.html#multinomial-naive-bayes) keyword in the Bayesian classifier is the \"smoothing parameter\" that we discussed in class -- increasing the value decreases the sensitivity to any single feature, and tends to pull prediction probabilities closer to 50%. \n",
    "\n",
    "How are we are going to use cross-validation to tune these hyperparameters?  The objective function we want to maximize is the log-likelihood of our data.  Fill in the remaining code in this block, to loop over many values of `alpha` and `min_df` to determine which settings are \"best\" in the sense of maximizing the cross-validated log-likelihood.\n",
    "\n",
    "How does the choice of these hyperparameters affect the performance of the model?\n",
    "\n",
    "*Hints:* \n",
    "* sklearn has a built-in function, `sklearn.cross_validation.cross_val_score`, that might save you a lot of time here. You can use 5-fold CV. If you go this route, you'll want to set the scoring function to be the custom likelihood function defined earlier.\n",
    "* For CV we only use the training set. This means in every iteration, for every combination of alpha and min_df, you will need to re-split into training and test sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha is : 0\n",
      "min_df is : 0.0001\n",
      "log scores: -12041.87\n",
      "min_df is : 0.001\n",
      "log scores: -1969.26\n",
      "min_df is : 0.01\n",
      "log scores: -1266.58\n",
      "min_df is : 0.1\n",
      "log scores: -1323.94\n",
      "min_df is : 0.2\n",
      "log scores: -1330.68\n",
      "alpha is : 1\n",
      "min_df is : 0.0001\n",
      "log scores: -1126.97\n",
      "min_df is : 0.001\n",
      "log scores: -1150.83\n",
      "min_df is : 0.01\n",
      "log scores: -1265.55\n",
      "min_df is : 0.1\n",
      "log scores: -1323.94\n",
      "min_df is : 0.2\n",
      "log scores: -1330.68\n",
      "alpha is : 5\n",
      "min_df is : 0.0001\n",
      "log scores: -1478.95\n",
      "min_df is : 0.001\n",
      "log scores: -1095.86\n",
      "min_df is : 0.01\n",
      "log scores: -1262.56\n",
      "min_df is : 0.1\n",
      "log scores: -1323.91\n",
      "min_df is : 0.2\n",
      "log scores: -1330.69\n",
      "alpha is : 10\n",
      "min_df is : 0.0001\n",
      "log scores: -2138.36\n",
      "min_df is : 0.001\n",
      "log scores: -1138.96\n",
      "min_df is : 0.01\n",
      "log scores: -1260.68\n",
      "min_df is : 0.1\n",
      "log scores: -1323.89\n",
      "min_df is : 0.2\n",
      "log scores: -1330.69\n",
      "alpha is : 50\n",
      "min_df is : 0.0001\n",
      "log scores: -3192.36\n",
      "min_df is : 0.001\n",
      "log scores: -1949.58\n",
      "min_df is : 0.01\n",
      "log scores: -1280.73\n",
      "min_df is : 0.1\n",
      "log scores: -1323.94\n",
      "min_df is : 0.2\n",
      "log scores: -1330.74\n",
      "best alpha, best min_dfs:  5 0.001\n"
     ]
    }
   ],
   "source": [
    "#You will want to suppress the warnings, since there might be a lot of them in this block. If you want to view them, change\n",
    "# the 'ignore' to 'default' or 'once'\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "#the grid of parameters to search over\n",
    "alphas = [0, 1, 5, 10, 50]\n",
    "min_dfs = [0.0001,0.001,0.01, 0.1, 0.2]\n",
    "\n",
    "#Find the best value for alpha and min_df, and the best classifier\n",
    "best_alpha = None\n",
    "best_min_df = None\n",
    "max_loglike = -np.inf\n",
    "\n",
    "for alpha in alphas:\n",
    "    \n",
    "    print('alpha is : ' + str(alpha))\n",
    "    \n",
    "    for min_df in min_dfs:\n",
    "        \n",
    "        print('min_df is : ' + str(min_df))\n",
    "        \n",
    "        #initialize vecotizer \n",
    "        vectorizer = CountVectorizer(min_df = min_df)       \n",
    "        X, Y = make_xy(reviews, vectorizer)\n",
    "        \n",
    "        #train test split\n",
    "        X_train, X_test, y_train, y_test= train_test_split(X, Y, test_size= .2, random_state = 42, stratify= Y)\n",
    "        \n",
    "        #initialize NB model\n",
    "        clf = MultinomialNB(alpha = alpha)\n",
    "\n",
    "        #cross validating the NB model using 5 cv\n",
    "        score = cross_val_score(clf, X_train, y_train, cv=5, scoring = log_likelihood)\n",
    "        mean_score = score.mean()\n",
    "        print('log scores: {:.2f}'.format(np.mean(score)))\n",
    "        \n",
    "        #updating the best parameters\n",
    "        if mean_score > max_loglike:\n",
    "            max_loglike = np.mean(score)\n",
    "            best_alpha = alpha\n",
    "            best_min_df = min_df\n",
    "\n",
    "print(\"best alpha, best min_dfs: \",best_alpha, best_min_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does the choice of these hyperparameters affect the performance of the model?\n",
    "- its the combination of both alpha and min_df that yields to different accuracy score. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Putting it together\n",
    "\n",
    "Now that you've determined values for alpha and min_df that optimize the cross-validated log-likelihood, repeat the steps in 2.1-2.3 to train a final classifier with these parameters and re-evaluate the accuracy on training and test sets.  Discuss the various ways in which cross-validation has affected the model. Is the new model more or less accurate? Is overfitting better or worse?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set Accuracy Score: \n",
      " 79.72282288185572\n",
      "Testing set Accuracy Score: \n",
      " 72.6808176100629\n"
     ]
    }
   ],
   "source": [
    "best_alpha = 5\n",
    "best_min_df = 0.001\n",
    "\n",
    "#final vectorizer model and X, Y data\n",
    "vectorizer_final = CountVectorizer(min_df = best_min_df)       \n",
    "X_final, Y_final = make_xy(reviews, vectorizer_final)\n",
    "\n",
    "#train-test split\n",
    "X_train_final, X_test_final, y_train_final, y_test_final = train_test_split(X_final, Y_final, test_size= .2, random_state = 42, stratify= Y_final)\n",
    "\n",
    "#fit NB model on the training datasets\n",
    "clf_final = MultinomialNB(alpha = best_alpha)\n",
    "clf_final.fit(X_train_final, y_train_final)\n",
    "\n",
    "#predict y on training data\n",
    "y_train_pred = clf_final.predict(X_train_final)\n",
    "\n",
    "#predict y on testing data\n",
    "y_test_pred = clf_final.predict(X_test_final)\n",
    "\n",
    "#accuracy score for training and testing predictions\n",
    "test_pred_score = accuracy_score(y_test_final, y_test_pred)\n",
    "train_pred_score = accuracy_score(y_train_final, y_train_pred)\n",
    "\n",
    "print('Training set Accuracy Score: \\n', (100 * train_pred_score))\n",
    "print('Testing set Accuracy Score: \\n', (100 * test_pred_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The overall accuracy score improved slightly. The training accuracy 79.72% is higher than the testing  accuracy 72.68%. Overfitting is bad since we might be losing information and accuracy on the actual testing data if we overfit on the training data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 (EXTRA CREDIT) \n",
    "\n",
    "What happens if you tried this again using a function besides the log-likelihood -- for example, the classification accuracy?  Interpret these results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha is : 0\n",
      "min_df is : 0.0001\n",
      "accuracy scores: 0.70\n",
      "min_df is : 0.001\n",
      "accuracy scores: 0.72\n",
      "min_df is : 0.01\n",
      "accuracy scores: 0.61\n",
      "min_df is : 0.1\n",
      "accuracy scores: 0.51\n",
      "min_df is : 0.2\n",
      "accuracy scores: 0.51\n",
      "alpha is : 1\n",
      "min_df is : 0.0001\n",
      "accuracy scores: 0.75\n",
      "min_df is : 0.001\n",
      "accuracy scores: 0.72\n",
      "min_df is : 0.01\n",
      "accuracy scores: 0.61\n",
      "min_df is : 0.1\n",
      "accuracy scores: 0.52\n",
      "min_df is : 0.2\n",
      "accuracy scores: 0.51\n",
      "alpha is : 5\n",
      "min_df is : 0.0001\n",
      "accuracy scores: 0.63\n",
      "min_df is : 0.001\n",
      "accuracy scores: 0.71\n",
      "min_df is : 0.01\n",
      "accuracy scores: 0.61\n",
      "min_df is : 0.1\n",
      "accuracy scores: 0.51\n",
      "min_df is : 0.2\n",
      "accuracy scores: 0.51\n",
      "alpha is : 10\n",
      "min_df is : 0.0001\n",
      "accuracy scores: 0.54\n",
      "min_df is : 0.001\n",
      "accuracy scores: 0.67\n",
      "min_df is : 0.01\n",
      "accuracy scores: 0.61\n",
      "min_df is : 0.1\n",
      "accuracy scores: 0.51\n",
      "min_df is : 0.2\n",
      "accuracy scores: 0.51\n",
      "alpha is : 50\n",
      "min_df is : 0.0001\n",
      "accuracy scores: 0.50\n",
      "min_df is : 0.001\n",
      "accuracy scores: 0.51\n",
      "min_df is : 0.01\n",
      "accuracy scores: 0.58\n",
      "min_df is : 0.1\n",
      "accuracy scores: 0.51\n",
      "min_df is : 0.2\n",
      "accuracy scores: 0.51\n",
      "best alpha, best min_dfs:  1 0.0001\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import balanced_accuracy_score, make_scorer\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#the grid of parameters to search over\n",
    "alphas = [0, 1, 5, 10, 50]\n",
    "min_dfs = [0.0001,0.001,0.01, 0.1, 0.2]\n",
    "\n",
    "#Find the best value for alpha and min_df, and the best classifier\n",
    "best_alpha = None\n",
    "best_min_df = None\n",
    "max_accuracy_score = -np.inf\n",
    "\n",
    "for alpha in alphas:\n",
    "    \n",
    "    print('alpha is : ' + str(alpha))\n",
    "    \n",
    "    for min_df in min_dfs:\n",
    "        \n",
    "        print('min_df is : ' + str(min_df))\n",
    "        \n",
    "        #initialize vecotizer \n",
    "        vectorizer = CountVectorizer(min_df = min_df)       \n",
    "        X, Y = make_xy(reviews, vectorizer)\n",
    "        \n",
    "        #train test split\n",
    "        X_train, X_test, y_train, y_test= train_test_split(X, Y, test_size= .2, random_state = 42, stratify= Y)\n",
    "        \n",
    "        #initialize NB model\n",
    "        clf = MultinomialNB(alpha = alpha)\n",
    "\n",
    "        #cross validating the NB model using 5 cv\n",
    "        scorer = make_scorer(balanced_accuracy_score)\n",
    "        score = cross_val_score(clf, X_train, y_train, cv=5, scoring = scorer)\n",
    "        mean_score = score.mean()\n",
    "        print('accuracy scores: {:.2f}'.format(np.mean(score)))\n",
    "        \n",
    "        #updating the best parameters\n",
    "        if mean_score > max_accuracy_score:\n",
    "            max_accuracy_score = np.mean(score)\n",
    "            best_alpha = alpha\n",
    "            best_min_df = min_df\n",
    "\n",
    "print(\"best alpha, best min_dfs: \",best_alpha, best_min_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set Accuracy Score: \n",
      " 90.819736583448\n",
      "Testing set Accuracy Score: \n",
      " 76.41509433962264\n"
     ]
    }
   ],
   "source": [
    "#final vectorizer model and X, Y data\n",
    "vectorizer_final_2 = CountVectorizer(min_df = 0.0001)       \n",
    "X_final_2, Y_final_2 = make_xy(reviews, vectorizer_final_2)\n",
    "\n",
    "#train-test split\n",
    "X_train_final_2, X_test_final_2, y_train_final_2, y_test_final_2 = train_test_split(X_final_2, Y_final_2, test_size= .2, random_state = 42, stratify= Y_final_2)\n",
    "\n",
    "#fit NB model on the training datasets\n",
    "clf_final_2 = MultinomialNB(alpha = 1)\n",
    "clf_final_2.fit(X_train_final_2, y_train_final_2)\n",
    "\n",
    "#predict y on training data\n",
    "y_train_pred_2 = clf_final_2.predict(X_train_final_2)\n",
    "\n",
    "#predict y on testing data\n",
    "y_test_pred_2 = clf_final_2.predict(X_test_final_2)\n",
    "\n",
    "#accuracy score for training and testing predictions\n",
    "test_pred_score_2 = accuracy_score(y_test_final_2, y_test_pred_2)\n",
    "train_pred_score_2 = accuracy_score(y_train_final_2, y_train_pred_2)\n",
    "\n",
    "print('Training set Accuracy Score: \\n', (100 * train_pred_score_2))\n",
    "print('Testing set Accuracy Score: \\n', (100 * test_pred_score_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a different accuracy function we got different optimun alpha and min_df value. Potentially we have a overfitting on the training data, since the training accuracy is a lot higher than the testing set data.\n",
    "\n",
    "Overally, this new set of min_df and alpha value increased the accuracy score. The testing data accuracy improved from 72.68% to 76.415%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Interpretation\n",
    "\n",
    "What words best predict a fresh or rotten review?  Using your classifier and the `vectorizer.get_feature_names` method (classifier and vectorizer from 3.3), determine which words best predict a positive or negative review. Print the 10 words that best predict a \"fresh\" review, and the 10 words that best predict a \"rotten\" review. For each word, what is the model's probability of freshness if the word appears one time?\n",
    "\n",
    "#### Hints\n",
    "* In thinking about how to measure the impact of a word on freshness rating, consider computing the classification probability for a feature vector which consists of all 0s, except for a single 1. What does this probability refer to?\n",
    "* `numpy.identity` generates an identity matrix, where all values are zero except the diagonal elements which have a value of one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>2216</th>\n",
       "      <th>2217</th>\n",
       "      <th>2218</th>\n",
       "      <th>2219</th>\n",
       "      <th>2220</th>\n",
       "      <th>2221</th>\n",
       "      <th>2222</th>\n",
       "      <th>2223</th>\n",
       "      <th>2224</th>\n",
       "      <th>2225</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12713</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12714</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12715</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12716</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12717</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12718 rows × 2226 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0     1     2     3     4     5     6     7     8     9     ...  2216  \\\n",
       "0         0     0     0     0     0     0     0     0     0     0  ...     0   \n",
       "1         0     0     0     0     0     0     0     0     0     0  ...     0   \n",
       "2         0     0     0     0     0     0     0     0     0     0  ...     0   \n",
       "3         0     0     0     0     0     0     0     0     0     0  ...     0   \n",
       "4         0     0     0     0     0     0     0     0     0     0  ...     0   \n",
       "...     ...   ...   ...   ...   ...   ...   ...   ...   ...   ...  ...   ...   \n",
       "12713     0     0     0     0     0     0     0     0     0     0  ...     0   \n",
       "12714     0     0     0     0     0     0     0     0     0     0  ...     0   \n",
       "12715     0     0     0     0     0     0     0     0     0     0  ...     0   \n",
       "12716     0     0     0     0     0     0     0     0     0     0  ...     0   \n",
       "12717     0     0     0     0     0     0     0     0     0     0  ...     0   \n",
       "\n",
       "       2217  2218  2219  2220  2221  2222  2223  2224  2225  \n",
       "0         0     0     1     0     0     0     0     0     0  \n",
       "1         0     0     0     0     0     0     0     0     0  \n",
       "2         0     0     0     0     0     0     0     0     0  \n",
       "3         0     0     0     0     0     0     0     0     0  \n",
       "4         0     0     0     0     0     0     0     0     0  \n",
       "...     ...   ...   ...   ...   ...   ...   ...   ...   ...  \n",
       "12713     0     0     1     0     0     0     0     0     0  \n",
       "12714     0     0     0     0     0     0     0     0     0  \n",
       "12715     0     0     0     0     0     0     0     0     0  \n",
       "12716     0     0     0     0     0     0     0     0     0  \n",
       "12717     0     0     0     0     0     0     0     0     0  \n",
       "\n",
       "[12718 rows x 2226 columns]"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_final_df = pd.DataFrame(X_final)\n",
    "X_final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as ppool\n",
    "\n",
    "#identity matrix with the size of the features\n",
    "a = ppool.identity(12718).astype('int64')\n",
    "proba_a = clf_final.predict_proba(X_final)\n",
    "word_prob_df = pd.DataFrame(proba_a, columns= ['Rotten', 'fresh'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rotten</th>\n",
       "      <th>fresh</th>\n",
       "      <th>word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.078096</td>\n",
       "      <td>0.921904</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.104250</td>\n",
       "      <td>0.895750</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.064784</td>\n",
       "      <td>0.935216</td>\n",
       "      <td>1961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.010706</td>\n",
       "      <td>0.989294</td>\n",
       "      <td>1998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.018696</td>\n",
       "      <td>0.981304</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12713</th>\n",
       "      <td>0.007986</td>\n",
       "      <td>0.992014</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12714</th>\n",
       "      <td>0.883924</td>\n",
       "      <td>0.116076</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12715</th>\n",
       "      <td>0.857707</td>\n",
       "      <td>0.142293</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12716</th>\n",
       "      <td>0.061004</td>\n",
       "      <td>0.938996</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12717</th>\n",
       "      <td>0.701817</td>\n",
       "      <td>0.298183</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12718 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Rotten     fresh  word\n",
       "0      0.078096  0.921904    10\n",
       "1      0.104250  0.895750   100\n",
       "2      0.064784  0.935216  1961\n",
       "3      0.010706  0.989294  1998\n",
       "4      0.018696  0.981304    20\n",
       "...         ...       ...   ...\n",
       "12713  0.007986  0.992014   NaN\n",
       "12714  0.883924  0.116076   NaN\n",
       "12715  0.857707  0.142293   NaN\n",
       "12716  0.061004  0.938996   NaN\n",
       "12717  0.701817  0.298183   NaN\n",
       "\n",
       "[12718 rows x 3 columns]"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = vectorizer_final.get_feature_names()\n",
    "#output the df with the prob and word features\n",
    "features_series = pd.Series(features)\n",
    "word_prob_df['word'] = features_series\n",
    "word_prob_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rotten</th>\n",
       "      <th>fresh</th>\n",
       "      <th>word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.078096</td>\n",
       "      <td>0.921904</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.104250</td>\n",
       "      <td>0.895750</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.064784</td>\n",
       "      <td>0.935216</td>\n",
       "      <td>1961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.010706</td>\n",
       "      <td>0.989294</td>\n",
       "      <td>1998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.018696</td>\n",
       "      <td>0.981304</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2221</th>\n",
       "      <td>0.974334</td>\n",
       "      <td>0.025666</td>\n",
       "      <td>your</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2222</th>\n",
       "      <td>0.529130</td>\n",
       "      <td>0.470870</td>\n",
       "      <td>yourself</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2223</th>\n",
       "      <td>0.521391</td>\n",
       "      <td>0.478609</td>\n",
       "      <td>youth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2224</th>\n",
       "      <td>0.257037</td>\n",
       "      <td>0.742963</td>\n",
       "      <td>zemeckis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2225</th>\n",
       "      <td>0.262256</td>\n",
       "      <td>0.737744</td>\n",
       "      <td>zone</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2226 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Rotten     fresh      word\n",
       "0     0.078096  0.921904        10\n",
       "1     0.104250  0.895750       100\n",
       "2     0.064784  0.935216      1961\n",
       "3     0.010706  0.989294      1998\n",
       "4     0.018696  0.981304        20\n",
       "...        ...       ...       ...\n",
       "2221  0.974334  0.025666      your\n",
       "2222  0.529130  0.470870  yourself\n",
       "2223  0.521391  0.478609     youth\n",
       "2224  0.257037  0.742963  zemeckis\n",
       "2225  0.262256  0.737744      zone\n",
       "\n",
       "[2226 rows x 3 columns]"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_no_nan = word_prob_df.dropna(axis=0)\n",
    "word_no_nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rotten</th>\n",
       "      <th>fresh</th>\n",
       "      <th>word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1614</th>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.999984</td>\n",
       "      <td>rousing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>0.000158</td>\n",
       "      <td>0.999842</td>\n",
       "      <td>between</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.000402</td>\n",
       "      <td>0.999598</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1159</th>\n",
       "      <td>0.000415</td>\n",
       "      <td>0.999585</td>\n",
       "      <td>magic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1760</th>\n",
       "      <td>0.000422</td>\n",
       "      <td>0.999578</td>\n",
       "      <td>so</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1357</th>\n",
       "      <td>0.000442</td>\n",
       "      <td>0.999558</td>\n",
       "      <td>or</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>293</th>\n",
       "      <td>0.000483</td>\n",
       "      <td>0.999517</td>\n",
       "      <td>caught</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>308</th>\n",
       "      <td>0.000668</td>\n",
       "      <td>0.999332</td>\n",
       "      <td>charming</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1851</th>\n",
       "      <td>0.000674</td>\n",
       "      <td>0.999326</td>\n",
       "      <td>strong</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>990</th>\n",
       "      <td>0.000848</td>\n",
       "      <td>0.999152</td>\n",
       "      <td>innocence</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Rotten     fresh       word\n",
       "1614  0.000016  0.999984    rousing\n",
       "194   0.000158  0.999842    between\n",
       "5     0.000402  0.999598         30\n",
       "1159  0.000415  0.999585      magic\n",
       "1760  0.000422  0.999578         so\n",
       "1357  0.000442  0.999558         or\n",
       "293   0.000483  0.999517     caught\n",
       "308   0.000668  0.999332   charming\n",
       "1851  0.000674  0.999326     strong\n",
       "990   0.000848  0.999152  innocence"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#top 10 words that gave the fresh review\n",
    "top_fresh_pred = word_no_nan.sort_values(by = 'fresh', ascending= False)\n",
    "top_fresh_pred.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rotten</th>\n",
       "      <th>fresh</th>\n",
       "      <th>word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1103</th>\n",
       "      <td>0.999960</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>length</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1105</th>\n",
       "      <td>0.999433</td>\n",
       "      <td>0.000567</td>\n",
       "      <td>less</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1893</th>\n",
       "      <td>0.999120</td>\n",
       "      <td>0.000880</td>\n",
       "      <td>suspenseful</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>321</th>\n",
       "      <td>0.998897</td>\n",
       "      <td>0.001103</td>\n",
       "      <td>cinema</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>0.998819</td>\n",
       "      <td>0.001181</td>\n",
       "      <td>banal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1200</th>\n",
       "      <td>0.997648</td>\n",
       "      <td>0.002352</td>\n",
       "      <td>measure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1913</th>\n",
       "      <td>0.997383</td>\n",
       "      <td>0.002617</td>\n",
       "      <td>target</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008</th>\n",
       "      <td>0.997317</td>\n",
       "      <td>0.002683</td>\n",
       "      <td>travel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1128</th>\n",
       "      <td>0.996229</td>\n",
       "      <td>0.003771</td>\n",
       "      <td>little</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>882</th>\n",
       "      <td>0.996161</td>\n",
       "      <td>0.003839</td>\n",
       "      <td>hanks</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Rotten     fresh         word\n",
       "1103  0.999960  0.000040       length\n",
       "1105  0.999433  0.000567         less\n",
       "1893  0.999120  0.000880  suspenseful\n",
       "321   0.998897  0.001103       cinema\n",
       "158   0.998819  0.001181        banal\n",
       "1200  0.997648  0.002352      measure\n",
       "1913  0.997383  0.002617       target\n",
       "2008  0.997317  0.002683       travel\n",
       "1128  0.996229  0.003771       little\n",
       "882   0.996161  0.003839        hanks"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#top 10 words that gave the rotten review\n",
    "top_rotten_pred = word_no_nan.sort_values(by = 'Rotten', ascending= False)\n",
    "top_rotten_pred.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 Error Analysis\n",
    "\n",
    "One of the best sources for inspiration when trying to improve a model is to look at examples where the model performs poorly.  Find 5 fresh and rotten reviews where your model performs particularly poorly. Print each review.\n",
    "\n",
    "What do you notice about these mis-predictions? Naive Bayes classifiers assume that every word affects the probability independently of other words. In what way is this a bad assumption? In your answer, report your classifier's Freshness probability for the review \"This movie is not remarkable, touching, or superb in any way\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.07809573, 0.92190427],\n",
       "       [0.10425017, 0.89574983],\n",
       "       [0.0647839 , 0.9352161 ],\n",
       "       ...,\n",
       "       [0.85770743, 0.14229257],\n",
       "       [0.06100421, 0.93899579],\n",
       "       [0.70181714, 0.29818286]])"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#predict the final Y\n",
    "Y_pred_final = clf_final.predict(X_final)\n",
    "\n",
    "#get the probability distribution for Y prediction\n",
    "proba = clf_final.predict_proba(X_final)\n",
    "proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create df for the output and set the check as whether the prediction is correct or not\n",
    "data_error = pd.DataFrame(proba, columns= ['Rotten', 'fresh'])\n",
    "check = Y_pred_final  == Y_final\n",
    "\n",
    "data_error['y predicted'] = pd.DataFrame(Y_pred_final )\n",
    "data_error['y true'] = pd.DataFrame(Y_final)\n",
    "data_error['correct'] = pd.DataFrame(check)\n",
    "#data_error['word'] = features_series\n",
    "data_error['review'] = reviews['quote']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rotten</th>\n",
       "      <th>fresh</th>\n",
       "      <th>y predicted</th>\n",
       "      <th>y true</th>\n",
       "      <th>correct</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4878</th>\n",
       "      <td>0.002983</td>\n",
       "      <td>0.997017</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>An epic achievement in filmmaking.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5968</th>\n",
       "      <td>0.004184</td>\n",
       "      <td>0.995816</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>Scream builds to a splattering finale that sho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12684</th>\n",
       "      <td>0.006434</td>\n",
       "      <td>0.993566</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>The movie haunts you like a ballad whose tune ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5350</th>\n",
       "      <td>0.006670</td>\n",
       "      <td>0.993330</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>It isn't often that extremely clever moviemake...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6876</th>\n",
       "      <td>0.007649</td>\n",
       "      <td>0.992351</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>There has never been a film quite like Kasi Le...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Rotten     fresh  y predicted  y true  correct  \\\n",
       "4878   0.002983  0.997017            1       0    False   \n",
       "5968   0.004184  0.995816            1       0    False   \n",
       "12684  0.006434  0.993566            1       0    False   \n",
       "5350   0.006670  0.993330            1       0    False   \n",
       "6876   0.007649  0.992351            1       0    False   \n",
       "\n",
       "                                                  review  \n",
       "4878                  An epic achievement in filmmaking.  \n",
       "5968   Scream builds to a splattering finale that sho...  \n",
       "12684  The movie haunts you like a ballad whose tune ...  \n",
       "5350   It isn't often that extremely clever moviemake...  \n",
       "6876   There has never been a film quite like Kasi Le...  "
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#top 5 words that gave the wrong fresh prediction \n",
    "false_pred = data_error[data_error['correct'] == False]\n",
    "fals_pred_drop = false_pred.dropna(axis=0)\n",
    "\n",
    "false_fresh_pred = fals_pred_drop.sort_values(by = 'fresh', ascending= False)\n",
    "false_fresh_pred.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rotten</th>\n",
       "      <th>fresh</th>\n",
       "      <th>y predicted</th>\n",
       "      <th>y true</th>\n",
       "      <th>correct</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5125</th>\n",
       "      <td>0.996646</td>\n",
       "      <td>0.003354</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>Bergman's visually striking medieval morality ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2054</th>\n",
       "      <td>0.995534</td>\n",
       "      <td>0.004466</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>Where's John McClane when you need him? If not...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8154</th>\n",
       "      <td>0.994441</td>\n",
       "      <td>0.005559</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>The restoration eliminates nearly all the dist...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1647</th>\n",
       "      <td>0.994093</td>\n",
       "      <td>0.005907</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>Reduces the tumult of the last few decades to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>642</th>\n",
       "      <td>0.991052</td>\n",
       "      <td>0.008948</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>Man of the Year isn't Movie of the Year. It mi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Rotten     fresh  y predicted  y true  correct  \\\n",
       "5125  0.996646  0.003354            0       1    False   \n",
       "2054  0.995534  0.004466            0       1    False   \n",
       "8154  0.994441  0.005559            0       1    False   \n",
       "1647  0.994093  0.005907            0       1    False   \n",
       "642   0.991052  0.008948            0       1    False   \n",
       "\n",
       "                                                 review  \n",
       "5125  Bergman's visually striking medieval morality ...  \n",
       "2054  Where's John McClane when you need him? If not...  \n",
       "8154  The restoration eliminates nearly all the dist...  \n",
       "1647  Reduces the tumult of the last few decades to ...  \n",
       "642   Man of the Year isn't Movie of the Year. It mi...  "
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#top 5 words that gave the wrong rotten prediction \n",
    "false_rotten_pred = fals_pred_drop.sort_values(by = 'Rotten', ascending= False)\n",
    "false_rotten_pred.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformed text vector is \n",
      " [[0 0 0 ... 0 0 0]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "s = \"This movie is not remarkable, touching, or superb in any way\"\n",
    "# call `transform` to convert text to a bag of words\n",
    "s_t = vectorizer_final.transform([s]).toarray()\n",
    "\n",
    "print(\"Transformed text vector is \\n\", s_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.02231255, 0.97768745]])"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proba_s = clf_final.predict_proba(s_t)\n",
    "proba_s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- What do you notice about these mis-predictions? \n",
    "    That most of the comments have to be interpreted within context, and even some of them contains strong negative/positive word, it could means the opposite based on the sentence structure. Therefore, it's hard for model to make precise prediction solely based on independent words' probability\n",
    "\n",
    "\n",
    "- In what way is this a bad assumption? \n",
    "    Because the Naive Bayes classifiers assume every word affects the freshness probability independently of other words, some words are more importent in context than the others, for example, in the phase 'not good', the word 'not' is more important than 'good', but if we give each word an eqaul probability, than it might make a wrong prediction based sorely on the word 'good'. \n",
    "\n",
    "\n",
    "- In your answer, report your classifier's Freshness probability: 97.76%. which cause problems since the weighting on the word 'not' should be dominant, even there're multiple positive words present. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.7 (EXTRA CREDIT)  Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try something different now.\n",
    "\n",
    "[Word2Vec](https://en.wikipedia.org/wiki/Word2vec) is a popular algorithm based on a shallow neural network, that can use large amounts of unannotated plain text, to learn relationships between words automatically. This is often very useful in recommender systems, text tagging or machine translation. Let's try to implement a basic version of this for our reviews data and observe some interesting (and natural) relationships between words. \n",
    "\n",
    "You will be using a new dataset which contains IMDB reviews, since we need a sufficiently large dataset and the Rotten Tomatoes dataset we have won't suffice. Download the data from [here](https://www.kaggle.com/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews). Train a Word2Vec model on these reviews and try to do the following - \n",
    "\n",
    "* From 3.5, pick 3 words that best predict a \"fresh\" review, and another 3 words that best predict a \"rotten\" review. For each of these words, find and print the top 10 most similar words from the vocabulary. \n",
    "* Spot the odd one out! Use the model to find the odd word from each of these input lists of words- \n",
    "    * 'amazing','great','awesome','horrible'\n",
    "    * 'movie','cinema','theater','box'\n",
    "    * 'car','bike','ball','bus'\n",
    "    \n",
    "    \n",
    "Do you think this model is able to learn representations of words within a given context? Feel free to play around with the model and report any other interesting findings.\n",
    "\n",
    "\n",
    "NOTE - \n",
    "* You should use the [*gensim*](https://radimrehurek.com/gensim/index.html) library for this question. [This](https://radimrehurek.com/gensim/auto_examples/tutorials/run_word2vec.html) link contains all the details you need to get familarized with Word2Vec.\n",
    "* It is highly recommended to pre-process your input data before feeding it to the model. Most common pre-processing steps include changing to lower case, removing punctuations, removing non-alphabetic tokens and removing stop words. You may find the [*nltk*](https://www.nltk.org/) library very useful for this, but feel free to use any other library. You can run the below block to import these libraries (you may have to pip install if you do not have these already installed).\n",
    "* gensim provides access to pre-trained models ; **do not use a pre-trained model**. You should train the model yourself, and this will not take too long to run.\n",
    "* We recommend you set an embedding size of 100-200 and a window of 4 or 5 for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import gensim\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter your code here"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
